# エンコーディングと進化

- ニーズの変更などの要因でアプリケーションが変わることがある
- これはコードとデータが変わるということ
- そして、データは新旧バージョンが共存する時間帯があることがある

  - 段階的なアップデートなどをするとこういうことは起きる(しょうがない)

- 後方互換性と前方互換性があり、後方互換性は用心すれば問題ないが、前方互換性は古いコードが新しいコードを無視できる様にすることなので、なかなか難しい
- JSON,XML,Protoclo Buffers などがどの様にデータをエンコードするのか、そして、どの様にスキーマの変更をするのかが本章の話

## データエンコードのフォーマット

- 通常プログラムはデータを少なくとも 2 つの異なる表現で扱う

1. メモリ内ではデータはオブジェクト、構造体、リスト、配列、ハッシュテーブル、ツリーなどで保持される
   - これらのデータ構造は CPU によるアクセスや操作が効率的になるように最適化されている
   - 通常はポインタが使われる
1. ファイルにデータを書いたり、ネットワーク経由でデータを送信したりしたい場合は、データを何らかの自己完結しているバイトの並びとしてエンコードしないといけない
   - ポインタは他のプロセスには全く意味をなさないので、このバイトの並びの表現はメモリ内で通常使われるデータ構造とは全く異なるものになる

- つまりこの 2 つの表現の間で何らかの変換が必要になる
  - インメモリの表現からバイトの並びへの変換はエンコーディング(シリアライゼーション、マーシャリング),その逆はデコーディング(パース、デシリアライズ、アンマーシャリング)と呼ばれる

### 言語固有のフォーマット

- 多くのプログラミング言語には、インメモリオブジェクトをバイトの並びにエンコードする機能が組み込まれている
- これらのエンコーディングライブラリは、最小限のコードだけでインメモリのオブジェクトの保存とリストアを行えるので非常に便利です。とはいえ、これらには根の深い問題がある

- これらのエンコーディングは特定のプログラミング言語と密接に関係しており、他の言語でデータを読むのが難しい。
- こういったエンコーディングでデータを保存したり転送したりするということは、その時点で使っているプログラミング言語を長い期間にわたって使い続けるということであり、現在のシステムを他の組織と結合するのを難しくする
- 同じオブジェクト型でデータをリストアしようとするなら、デコーディングのプロセスは仕意のクラスをインスタンス化できなければならない
- これはセキュリティ上の問題の発生源になることが頻繁にある
  - 仮に攻撃者がアプリケーションに任意のバイト列をデコードさせられるなら、それは攻撃者が任意のクラスをインスタンス化できるということであり、すなわち任意のコードをリモートで実行するといったひどい行為を行えるということ
- こういったライブラリでは、多くの場合データのバージョニングが後付けになる。
  - これらのライブラリはデータを手早く簡単にエンコードできることを意図したものなので、前方及び後方互換性の不便な問題には目をつぶっていることが多い
  - 効率性（エンコードやデコードにかかる CPU 時間やエンコード後の構造のサイズ）もまた多くの場合後付けで考えられている
- 以上の理由から、ごく一時的な目的の場合を除けば、特定の言語の組み込みエンコーディングを使うのは良くない考え

### JSON、XML、様々なバイナリフォーマット

- JSON に人気があるのは、主に Web ブラウザで最初からサポートされている
  - JavaScript のサブセットであるという長所もある
- CSV もまた広く使われている言語から独立しているフォーマットだが、それほど強力ではない
- 数値のエンコーディングには多くの腰味さがあり,XML と CSV では、数値とたまたま数字だけから構成される文字列とを区別できない
- JSON は文字列と数値を区別するが、数値と浮動小数点数値は区別せず、精度を指定することもできない
- これは大きな数値を扱うときに問題になる。たとえば 253 は IEE 754 の倍精度浮動小数点数では正確に表現できないので、浮動小数点数を扱う言語（たとえば JavaScript）でパースすると、こういった数値は不正確になる。

  - Twitter の API が返す JSON にはツイートの ID が 2 つ含まれており、1 つは JSON の数値型に、もう 1 つは 10 進数の文字列型になっており、これは、こういった数値が JavaScript のアプリケーションでは正確にパースされないことに対する回避策

- JSON と XML はユニコード文字列のサポート機能が優れているが、バイナリ文字列をサポートしていない。
- バイナリ文字列は便利な機能なので、この制限を回避するのに Base64 を使ってバイナリデータをエンコードするという方法が使われる。
- そして、その値が Base64 エンコードされたものであることを示すためにスキーマが使われる。
- この方法はうまくいくものの、多少手が込んでいる上にデータサイズが 33%増加してしまう
- XML, JSON では、それぞれのフォーマット本体とは別に、利用できるスキーマがある
- これらのスキーマ言語はきわめて強力であり、それ故に学んで実際に利用するまではきわめて複雑
- XML スキーマは広く利用されていますが、JSON ベースのツールの多くはスキーマを使うようなことはしていない。
- データを正しく解釈できるか（数値やバイナリ文字列など）はスキーマの情報によるので、XML/JSON スキーマを使わないアプリケーションは適切なエンコーディング/デコーディングのロジックをハードコーディングしなければならないかもしれない。
- CSV にはスキーマがないので、それぞれの列や行の意味の定義はアプリケーションに任されている。
  - CSV は非常に曖味なフォーマット
  - CSV でのエスケープのルールは正式に規定されているが、すべてのパーサでそれが正しく実装されているわけではない

### バイナリエンコーディング

- テラバイト級 のデータであればデータフォーマットの選択の影響は大きくなる
- バイナリフォーマットは JSON や XML に比べてデータサイズが小さく、パースも高速

### Thrift & Protocol Buffers

- Apache Thrift と Protocol Buffers は、同じ原理に基づくバイナリエンコーディングライブラリ
- もともと Protocol Buffers は Google で、Thrift は Facebook で開発されたライブラリであり、どちらも 2007 年から 2008 年にオープンソース化された
- Thrift と Protocol Buffers は、どちらもエンコードするデータに対するスキーマを必要とする
- Thrift と Protocol Buffers には、スキーマ定義を受けて様々なプログラミング言語でコードを生成し、スキーマを実装したクラスを生成してくれるツールがある
- アプリケーションのコードは、この生成されたコードを呼び出してこのスキーマのレコードのエンコードやデコードを行える

### フィールドタグとスキーマの進化

スキーマが時間の経過とともに変化せざるを得ないものであることは述べました。この変化はスキーマの進化（schema evolution）と呼ばれます。Thrift や Protocol Buffers は、どのようにして後方及び前方互換性を保ちつつスキーマの変化を扱うのでしょうか？
本章のサンプルから分かるとおり、エンコードされたレコードは単にエンコードされたフィールドをつなげていっただけのものに過ぎません。それぞれのフィールドはタグ番号 （サンプルのスキーマ中の 1、2、3）で識別され、データ型が付けられます（たとえば文字列型や整数型）。フィールドの値に値が設定されていなければ、単純にそのフィールドはエンコード後のレコードには含まれないことになります。このことから分かるのは、エンコードされたデータの意味にとってフィールドタグがきわめて重要だということです。エンコードされたデータがフィールド名を参照することはないので、スキーマ中のフィールド名を変更することはできますが、フィールドのタグは変更できません。これはタグを変更するとエンコード済みの既存の全データが不正なものになってしまうためです。
新しいタグ番号を使うかぎりにおいて、スキーマには新しいフィールドを加えていくことができます。仮に古いコード（このコードは追加された新しいタグ番号のことは知りません）が新しいコードによって書かれたデータを読み取ろうとした場合、古いコードは認識できないタグ番号を持つ新しいフィールドを単純に無視します。データ型のアノテーションを見れば、パーサはスキップしなければならないバイト数を判断できます。これで新しいコードによって書かれたデータを古いコードが読めるので、前方互換性が保たれます。
後方互換性についてはどうでしょうか。それぞれのフィールドがユニークなタク番号を持ってき
えいれば、タグ番号の意味が変わることはないので新しいコードは常に古いデータを読むことができます。細かな部分で 1 つだけ注意が必要なのは、新しく追加するフィールドは必須にできないことです。新しいフィールドを追加して必須にしてしまえば、古いコードは追加されたフィールドをデータに書いていないので、古いコードが書いたデータを新しいコードが読んだときのチェックは失敗してしまいます。したがって後方互換性を保つためには、スキーマを最初にデプロイした後に追加するすべてのフィールドを必須としないか、初期値を指定するかしなければなりません。
フィールドの削除も追加と同様ですが、後方及び前方互換性の扱いが逆になります。すなわち削除できるのはオプションとして指定されているフィールド（必須のフィールドは削除できません）
だけであり、同じタグ番号を使い回すことはできません（古いタグ番号を含んで書かれたデータがどこかに残っているかもしれず、新しいコードはそのタグ番号のフィールドを無視しなければなり
ません）。

### データ型とスキーマの進化

フィールドのデータ型の変更についてはどうでしょうか？、型を変更できる場合もありますが（時縮はそれぞれのエンコーディングのドキュメンテーションを開べてください）、値の精度が低くなったり、切り捨てられてしまったりするリスクはあります。たとえば 325t の繋数を 64bit の茎数に変更したとしましょう。この場合、パーサは足りないピットを 0 で埋めてしまえるので、新しいコードは問題なく古いコードによって書かれたデータを読めます。しかし、仮に古いコードが新しいコードによって書かれたデータを読んだ場合、古いコードは依然として値を 32bit 長の変数に格納することになります。デコードされた 64bit 長の値が 32bit 長に収まらなかったら、その値は切り詰められてしまいます。
Protocol Bufers の細部で興味深いのは、リストや配列のデータを持たず、その代わりにフィールドに repeated マーカーがあることです（これは required と optional に並ぶ 3 番目のオプションです）。図 4-4 にあるように、repeated が指定されたフィールドのエンコーディングはそのまま同じフィールドタグが複数回レコード中に現れるだけです。このやり方には、optional（値は 1 つだけ）指定されたフィールドを repeated（値は複数）指定されたフィールドに変更できるようになるという喜ばしい効果もあります。古いデータを読む新しいコードは、0 もしくは 1 つの要素（どちらになるかはフィールドが実際に存在しているかどうかによります）を持つリストを見ることになります。新しいデータを読む古いコードは、リストの最後の要素だけを見ることになります。
Thnitt には専用のリストデータ型があります。これはリストの要素のデータ型をパラメータとして持ちます。この方法では、単一の値を持つフィールドから複数の値を持つフィールドへの進化を、Protool Bufets の場合と同じようには扱えませんが、ネストしたリストをサポートできるという利点があります。

### Avro

Amdle Dro もまたパイナリのエンコーディングフォーマットであり 120）、Protocol Bufes
うまくいとの違いが現状洗いところです。Apache ANTO は、Thit が Hadoop のユースケースにたまく場合しなかったことを受けて、2009 に Hadoop の副プロジェクトとして立ち上がりまし
システもます。コンロードするデータの報道を指定するのにスキーマを使います。ANTO には？つ

とろ質があり、1 つは人間が編集することを施したもの（ANTO IDL）、もう 1 つはものと機械が読み取りやすいもの（JSON ペースです）です。
本章のサンプルスキーマを AVIO IDL で書けば、以下のようになるでしょう。
何よりもまず、このスキーマにはタグ番号がないことに注目してください。このスキーマを使ってサンプルのレコード（例 4-1）をエンコードすると、AVrO のバイナリエンコーディングはわずか
32 バイトにしかなりません。これはこれまでに見たエンコーディングの中では最もコンパクトです。図 4-5 は、エンコードされたバイト列の分解図です。
このバイトの並びを調べてみれば、フィールドやフィールドのデータ型を示すものがないことが分かります。このエンコーディングに含まれているのは、連結された値だけなのです。文字列は単に長さのプレフィックスに続いて UTF-8 のバイトが続いているだけですが、そのエンコードされたデータが文字列であることを示すものはありません。データは数や、まったく別の何かかもしれないのです。整数のエンコーディングには可変長のエンコーディングが使われています（これは Thritt の CompactProtocol と同じです）。
このバイナリデータをパースするには、スキーマ中に現れる順序に従ってフィールドを見ていき、スキーマを使ってそれぞれのフィールドのデータ型を判断しなければなりません。これはすなわち、データを読み取るコードがバイナリデータを正しくデコードするためには、データが書き込まれたときと完全に同じスキーマを使わなければならないということです。リーダーとライターが使うスキーマに不一致があれば、データを正しくデコードできなくなってしまいます。
それでは、AVIO はどのようにスキーマの進化を扱うのでしょうか？

### ライターのスキーマとリーダーのスキーマ

AVIO では、アプリケーションがデータをエンコードする（ファイルやデータベースに書き込む、あるいはネットワーク越しにデータを送信するといったことのため）際に、アプリケーションが知っているいかなるバージョンのスキーマを使ってもかまいません。たとえばそのスキーマがアプリケーションにコンパイル時に組み込まれていてもよいのです。このスキーマはライターのスキーマと呼ばれます。
アプリケーションが何らかのデータ（ファイルやデータベースから読み取ったデータや、ネットワークから受信したデータ）をデコードしたい場合、そのデータは何らかのスキーマに従っていると考えられます。このスキーマはリーダーのスキーマと呼ばれます。アプリケーションコードが依存するのはこのスキーマで、アプリケーションのビルドプロセスでこのスキーマからコードが生成されることがあります。
AVTO において鍵となる考え方は、ライターのスキーマとリーダーのスキーマは同一である必要がなく、互換性さえあればよいという考え方です。データがデコード（読み込み）される際に、AVTO のライブラリはこの差異をライターのスキーマとリーダーのスキーマを並べて調べることによって解決し、ライターのスキーマからリーターのスキーマへデータを変換します。この AVO の仕様には 120）、この解決がどのように行われるのかが厳密に定義されています。図 4-6 はその様

です。
たとえばライターのスキーマとリーダーのスキーマの間でフィールドの順序が異なっていても問題はありません。これは、スキーマの解決の際にフィールド同士を名前でマッチさせるためです。
データを読み取るコードがライターのスキーマにはあり、リーダーのスキーマにはないフィールドに出会った場合は、そのフィールドは無視されます。リーダーのスキーマが期待するフィールドの名前がライターのスキーマには含まれていない場合は、そのフィールドにはリーダーのスキーマで管言されているデフォルト値が与えられます。

### スキーマの進化の規則

AVIO における前方互換性とは、新しいバージョンのスキーマをライターとして持ち、古いバージョンのスキーマをリーダーとして持てるということです。これとは逆に、後方互換性が意味するのは新しいバージョンのスキーマをリーダーとして持ち、古いバージョンのスキーマをライターとして持てるということです。
互換性を保つために、追加や削除できるのはデフォルト値を持っているフィールドのみです（本
章の AVrO のスキーマの favoriteNumber フィールドのデフォルト値は null になっています）。たとえばデフォルト値を持つフィールドを追加し、この新しいフィールドが新しいスキーマにのみ存在し、古いスキーマには存在しないようにしたとしましょう。古いスキーマを使って書かれたしコードを、リーダーが新しいスキーマを使って読み取ったとすれば、父けているフィールドにはこのデフォルト値が補われます。
デフォルト値なしで新しいフィールドを追加しようとすると、新しいリーダーは古いライターで昔かれたデータを読めなくなってしまうので、後方互換性が損なわれることになります。デフォルト値を持たないフィールドを削除しようとすると、古いリーダーは新しいライターが書いたデータを読めなくなってしまうので、前方互換性が損なわれることになります。
プログラミング言話によっては、すべての変数で nuLL をデフォルトとして受け入れることがで
きますが、10m0 はそうなっていません。フィールドを調にしたい場合は 1mion 製を使わなければなりません。たとえば intion mal， ong.string ♪ taeld、とすれは、それはその fad
が※、文字、のいずれかになることを示します。デフォルト値としていいてを指定できるのは、PaL が aion の選択肢のいずれかになっている場合のみですする。これは、デフォルトでをに ml が使えるようにするのに比べてやや長になりますが、ml にできるるのとそうでないものを明示することによって、バグを防ぐのに役立ちます（22］。
この結果、AITO には Protocol Bufers や Thit のような optional や required はありません
（その代わりに union 型とデフォルト値があります）。
ANTO が型変換できるかぎりにおいて、フィールドのデータ型は変更できます。フィールドの名前を変えることもできますが、やや複雑になります。リーダーのスキーマにはフィールド名のエイリアスを持たせられるので、古いライターのスキーマのフィールド名に対してエイリアスをマッチさせることができます。これはすなわち、フィールド名を変更しても後方互換性は保たれますが、前方互換性は保たれないということです。同様に、union 型の選択肢を追加しても後方互換性は保たれますが、前方互換性は損なわれてしまいます。

### そもそもライターのスキーマとは何なのか？

これまで素知らぬふりをしてきた重要な疑問が 1 つあります。すなわち、あるデータがエンコードされた際のライターのスキーマを、リーダーはどのようにして知るのでしょうか？単純にスキーマをすべてのレコードに含めることなどできません。なぜなら、エンコードされたレコードよりもスキーマの方がはるかに大きくなるので、バイナリエンコーディングをすることで節約できた領域をすべて無駄にしてしまうでしょう。
この疑間に対する答えは、AVIO の使われ方によります。いくつか例を挙げましょう。
大量のレコードを持つ大きなファイルの場合
これにに Hadop で使われる場合に一般的な ANO のユースケースであり、同じスキース
ずにうこされた何百万ものレコードが含まれる日大なファイルを保存するような場合でウイイった状況については 10 等で癒します）。この場合、そのファイルのライターが、こは、※の間に一度だけライターのスキーマを含めておくだけですみます。AInO の仕
いますそれが可能なファイルフォーマンド（オプシェクトコンテナラアイル）が定められて

個別にレコードが書かれるデータベースの場合
データベースの場合、異なるライターのスキーマを使って様々な時点で様々なレコードが書かれることがあります。この場合、すべてのレコードが同じレコードを持つものと見なすことはできません。最もシンプルな解決方法は、エンコードされたすべてのレコードの先頭にバージョン番号を含め、スキーマのバージョンのリストをデータベース中に保存しておき、データベースからバージョン番号に応じたライターのスキーマをフェッチすることです。そうして取り出したライターのスキーマを使えば、その後のレコードはデコードできることになります（たとえば Espresso がこの方法を使っています［231）。
ネットワーク経由でレコードを送信する場合
2 つのプロセスが双方向のネットワーク接続経由で通信している場合、それらのプロセスは接続の確立時にスキーマのバージョンに関するネゴシエーションを行い、接続が続く間そのスキーマを使うことができます。Avro RPC プロトコル（p.140「4.2.2 サービス経由でのデータフロー：REST と RPC」）はこの方法で動作します。
スキーマバージョンのデータベースは、ドキュメンテーションとしての役割を果たし、スキーマの互換性をチェックできるようになることから、どのようなユースケースでも役立ちます［24）。
バージョン番号としては単純に整数をインクリメントして使うことも、スキーマのハッシュ値を使うこともできるでしょう。

### 動的に生成されたスキーマ

Protocol Buffers や Thrift と比較した場合、AVIO の利点としてスキーマにタグ番号が含まれないということがあります。しかしなぜこのことが重要なのでしょうか。スキーマ中にいくつかの数値を保持しておくことがなぜ問題になるのでしょうか。
それは、AVIO が動的に生成されたスキーマとの相性が良いことにあります。たとえばリレーショナルデータベースの内容をファイルにダンプしたいものの、すでに述べたようなテキストのフォーマット（JSON,CSV、SQL）を使うことから生じる問題は避けたいとします。AVIO を使えば、リレーショナルなスキーマから AVIO のスキーマを容易に生成でき（すでに見た JSON での表現が使えます）、そのスキーマを使ってデータベースの内容をエンコードし、すべての内容を AVTO のオブジェクトコンテナファイルにダンプできます 125）。それぞれのデータベーステーブルに対してしコードのスキーマを生成すれば、それぞれの列はレコードのフイールドになります。データベースの列名は AVIO のフィールド名にマップされます。
さあ、データベースのスキーマが変更された（たとえばテーブルに列が 1 つ追加され、既存の列が 1 つ制された）としましょう。この場合、更新されたデータベーススキーマから新しい At のスキーマを生成し、そのスキーマでデータをエクスポートできます。データのエクスポートのプロセスについては、スキーマの変更を気にする必要はありません。単に実行のたびにスキーマの変換
を行えばいいだけです。新しいデータファイルを読めば、レコードのフィールドが変更されたことが分かります。とはいえウイールドは名前で同定されるので、更新されたライターのスキーマを古
いリーダーのスキーマと照らしあわせ続けることはできます。
これに対し、同じことを Thit や Protocol Bufers でやろうとした場合、フィールドタグの割り当てはおそらく手作業でやることになるでしょう。データベーススキーマが変更されるたびに、管理者は手作業でデータベースの列名からフィールドタグへのマッピングを更新しなければならないかもしれません（この処理は自動化できるかもしれませんが、スキーマを生成する際に過去に使われたワィールドタグを割り当ててしまわないよう、細心の注意が必要になります）。これは単に、こういったスキーマの動的生成は Thrift や Protocol Buffers では設計目標ではなく、一方で AvTo
では設計目標だったということです。

### コード生成と動的型付き言語

Thrift と Protocol Bufers はコード生成に依存しています。スキーマが定義できれば、そのスキーマを実装するコードを好きなプログラミング言語で生成できます。Java、C++、C#といった静的型付き言語では、デコードされたデータ用に効率的なインメモリ構造が利用でき、そういった
データ構造にアクセスするプログラムを書く際に型チェックや IDE の自動補完機能を利用できることになることから、これは有益です。
JavaScript や Ruby、 Python といった動的型付きのプログラミング言語ではコンパイル時の型チェッカーを通さないので、コード生成にはそれほどの意味はありません。コード生成はこれらの言語では喜ばれないことがしばしばですが、これはコード生成をしなければ明示的なコンパイルのステップを避けられるためです。さらに、動的に生成されたスキーマ（たとえばデータベースのテーブルから生成された AVIO のスキーマ）の場合、コード生成はデータへのアクセスに対する不必要な障害となります。
ANTO では、静的型付き言語のためにコード生成をすることもできますが、コード生成をまったくしなくてもうまく利用できます。オブジェクトコンテナファイルがあるなら（これにはライターのスキーマが埋め込まれています）、単純に AVTO のライブラリでそのファイルをオープンして、JSON ファイルを見るのと同じようにその中のデータを見ることができます。このファイルは、必要なメタデータがすべて含まれていることから自己記述です。
この特性は、Apacthe Pig のような助的理付き処理言語とあわせて使われる場合に非常に有益です（20）。Apadhle Pig では、単細に AVIO のファイルをオープンしてその内容を分析し、楽出されたデータセットを AVO フォーマットのファイルに書き出すということを、スキーマを意識すること
なく行えます。

### スキーマのメリット

すでに見てきたとおり、Protocol Buffers， Thit、Avro はいずれもバイナリエンコーディングフォーマントを記込するのにスキーマを使います。これらのスキーマ音器は、XMIL Schema や

SON Schema など詳細なバリデーションの規則をサポートしているもの（たとえば「この文字列
フィールドは以下の正規表現にマッチしなければならない」あるいは「このフィールドの数値は 0 から 100 の間でなければならない」など）よりもはるかにシンプルです。Protocol Bufers、Thift、Avro は実装や使い方がシンプルなことによって、きわめて広範囲のプログラミング言語でサポートされるまでに成長したのです。
これらのエンコーディングが基盤としている考え方は、決して新しいものではありません。たとえば 1984 年に初めて標準化されたスキーマ定義言語である ASN.1/27）は、これらのエンコーディングとの数多くの共通点を持っています。ASN.1 は様々なネットワークプロトコルの定義に利用され、そのバイナリエンコーディング（DER）は、今でも SSL 証明書（X.509）をエンコードする際に利用されています（28）。 ASN.1 は Protocol Buffers や Thrift のようにタグ番号を利用したスキーマの進化をサポートしています 129］。とはいえ ASN.1 は非常に複雑であり、ドキュメントもあまり書かれていないので、新しいアプリケーションにとってはおそらく良い選択肢ではないでしょう。
多くのデータシステムもまた、そのデータ用に何らかの独自のバイナリエンコーディングを実装しています。たとえば多くのリレーショナルデータベースは、独自のネットワークプロトコルでデータベースへのクエリの送信とレスポンスの受信を行えるようにしています。こういったプロトコルは概して特定のデータベースに対して固有のものであり、データベースのネットワークプロトコルからのレスポンスをデコードしてインメモリのデータ構造に変換するドライバ（たとえば
ODBC や JDBC API が利用されます）が、データベースベンダーから提供されています。
つまり JSON、XML、CSV といったテキスト形式のデータフォーマットは広く使われているものの、スキーマに基づくバイナリエンコーディングもまた有効な選択肢であることが分かります。
バイナリエンコーディングには優れた性質がいくつもあります。
•様々な「バイナリ JSON」よりもはるかにコンパクトになりうること。これはエンコードされたデータにフィールド名を含める必要がないためです。
・スキーマはドキュメンテーションの形態の 1 つとしての価値があり、デコードにはスキーマが必要になることから、常に最新の状態になっていることが保証できます（一方で、手作業でメンテナンスされているドキュメントは容易に実体から乖離してしまいます）。
・スキーマのデータベースを管理することで、スキーマの変更の前方及び後方互換性をデプロイに先立ってチェックできます。
•静的型付き言語のユーザーにとっては、コンパイル時の型チェックができるようになることから、スキーマからのコード生成は有益です。
まとめると、スキーマの進化はスキーマレス／スキーマオンリードの JSON データベースが提供するのと同様に、ある種の柔性をもたらしながらデータに関する保証を高め、より優れたツール
を提供してくれるのです。

4.2 データフローの形態
本茶の始めに、何らかのデータを、メモリを共有していない他のプロセスに送信する場合、たとえばネットワーク番由でデータを送したり、ファイルへデータを書き出したりする場合には、そのデータをバイト列へとエンコードしなければならないと述べました。そして、そのための様々なエンコーディングについて論じてきました。
前方及び後方互換性は、進化性（システムの様々な部分を独立してアップグレードできるようにし、すべてを一度に変更する必要をなくすことによって、変更を加えることを容易にする）にとって重要であることを述べました。互換性とは、データをエンコードするプロセスと、そのデータをデコードするプロセスとの関係性です。
データをあるプロセスから他のプロセスへ流す方法は数多く存在するので、こういった議論はきわめて指象的です。データをエンコードするのは誰で、デコードするのは誰でしょうか？この後本章では、プロセス間でデータが流れる方法の中で、一般的な方法を見ていきます。
• データベース経由（p.138「42.1 データベース経由でのデータフロー」参照）
・サービス時び出し経由（p.140「42.2 サービス経由でのデータフロー：REST と RPC」参照
非同期のメッセージパッシング経由（p.146 「4.23 メッセージパッシングによるデータン
ロー」参照）
4.2.1
データベース経由でのデータフロー
データベースでは、データベースにおき込みを行うプロセスがデータをエンコードし、データイラスからないたスがそのデークをデコードします。データペースにアクセスを行うプロセスはいつたけしか存在しないこともありますが、それはライターと同じプロセスがり、
8 日としてというだけです。この場合、データベースへの保存処理は将来の自分自身へのメッセージ送と考えられます。
が書き込んだ内容をデコードできなくなってしまいます。
ここではならです。そうしないと掃来の自分自身は以前に自分長して、データベースにおめかなるプロセスが目にアクセスするのはよくあることです。そ
れらのプロセスは、とりチカのためションやサービスの場合もあれば、単に同じサ
ビスの（スケーラビリテイを開きはのために部列に助休している）様なのインスタンスに過ぎないこともあります。どちらにレーションリートがランがな化している現境では、たとえばローリシリアップリレードで新しいパージとのよ。がデプロイされており、正新請みのインスタンスと本理新のインスタンスが支付することをもくするもののペースにアクセスしているプロセスに新
ヒスコードで助けするものとキリコンでのものおかすることになるでしょう。
これはすなわち、蒸しいパージョンのコードによってかたなデージベース中の性が、彼然としであれし続けている古いパージョンのコードによって後からされるからごれないたいこと
す。したがって、データベースの場合は前方互換性が決められることも頻繁にあります。
ただし、落とし穴はもう 1 つあります。レコードのスキーマにフィールドを追加し、新しいコードが新しいフィールドの値をデータベースに書き込んだとしましょう。そしてその後、古いバージョンのコード（このコードは新しいフィールドのことは知りません）がそのレコードを読み、更新し、書き戻したとします。この状況下での望ましい動作は、新しいフィールドのことを解釈できなくても、古いコードが新しいフィールドには手を加えないことです。
先に述べたエンコーディングのフォーマットは、このような未知のフィールドの保全をサポートしますが、図 4-7 に示すようにアプリケーションレベルでの配慮が必要な場合があります。たとえばデータベースの値をアプリケーションのモデルオブジェクトにデコードし、後にそれらのモデルオブジェクト群をエンコードし直したとすると、その変換の過程で未知のフィールドが失われてしまうかもしれません。この問題を解決することは難しくありません。単にこの問題が認識できてさえいればいいのです。

4.2.1.1 異なる値が異なる時刻に書き込まれるケース
数してデータベースにおいては、任意の値を任意のタイミングで更新できます。これは、5 ミリ
砂前に書かれた値と 5 年前に書かれた値が 1 つのデータベース中に共存することもあるということ

新バージョンのアプリケーションをデプロイする場合のなにもサーバーサイドのアプリケーションであれい）、数分のうちに古いバージョンを新しいですで発全に配き換えることもできるでしょう。しかしデータベースの内容については話が別です。5 年が経造したデータは、現法前におき意されていないかぎり元のエショーディングのままで使っています。このことは「データはコードよりも長生きする」と表現されることもあります。
データを新しいスキーマにあわせて書き直す（マイグレーションする）ことはできますが、大規糖なデータセットの場合は大きな負担になるので、ほとんどのデータベースではできるだけ避けています。ほとんどのリレーショナルデータベースでは、デフォルト値を nul てとして新しい列を追加するといったシンプルなスキーマの変更は、既存のデータを書き換えることなく行えるようになっています\*6。古い行が読まれる際には、ディスク上のエンコード済みのデータにけている列があれば、データベースが nuLL を補います。Linkedln のドキュメントデータベースである
Espresso はストレージに AvIo を使い、AVIO のスキーマ進化のルールを利用できるようにしています 123］。
スキーマの進化によって、データベースの下位のストレージには過去の様々なバージョンのスキーマでエンコードされたデータが含まれていても、データベース全体が単一のスキーマでエンコードされているかのように扱うことができます。
4.2.1.2 アーカイブストレージ
データベースからは、たとえばバックアップのため、あるいはデータウェアハウスへのロードのために（p.97「3.21 データウェアハウス」参照）、時おりスナップショットをとることがあるでしょう。この場合、仮にソースデータベース中のもとのエンコーディングに様々な時期のスキーマが混在していても、データのダンプは最新のスキーマでエンコードされるのが普通です。ともあれデータをコピーするので、一貫性を保ってデータのコピーをエンコードすることになるでしょう。
データのダンプは一度に書き出され、その後は変更されることがないので、この場合 AVO のオプジェクトコンテナファイルのようなフォーマットが適しています。あるいは Parquet のような分析に恋した別指向フォーマットも、こういったデータのエンコードには適しているでしょう（p.108
「3.3.1 列の圧縮」参照）。
10 章では、アーカイブストレージでのデータの利用についてさらに論じます。
4.2.2 サービス経由でのデータフロー：REST と RPC
方がらリーク等山で通信しなければならないプロセス酢がある場合、その通行には数多くのやますがあります。取も一般的なやり方は、クライアントとサーパーという 2 つの役間を設けることです。サーバーはネットワークに対して AP を公開し、クライアントはサーバーに接続してその API にリクエストを発行します。サーバーが公開する API はサービスと呼ばれます。
Web は次のように動作します。クライアント（Web ブラウザ）が Web サーバーにリクエストを発行し、GET リクエストによって HTML、CSS、JavaSeript、画像などをダウンロードします。そして POST リクエストによってサーバーにデータを投入します。API は、標準化されたプロトコルとデータフォーマット（HTTP， URL、 SSL/TLS、 HTML など）で構成されます。Web ブラウが、Web サーのサンドオートの作成者はおおよそこれらの標準に同意しているので、仕送のプラウザで任意のサイトに（少なくとる理論的には）アクセスできます。
クライアントの種類は、Web ブラウザだけというわけではありません。たとえばモバイルデバイスやデスクトップコンピューター上で動作するネイティブのアプリケーションもサーバーに対してネットワークリクエストを発行し、Web ブラウザ内で動作するクライアントサイドの JavaScript
アプリケーションも XMLHttpRequest を使って HTTP クライアントになります（この手法は Ajax と呼ばれます［30］）。この場合、通常サーバーのレスポンスは人間に表示するための HTML ではなく、クライアントサイドのアプリケーションが処理しやすいフォーマット（JSON など）でエンコードされたデータになります。転送プロトコルとしては HTTP が使われることもありますが、最上位の API 実装はアプリケーションに固有のものであり、その API の詳細についてはクライアントとサーバーが合意していなければなりません。
さらに、サーバーそのものも他のサービスのクライアントになることがあります（たとえば典型的な Web アプリケーションのサーバーは、データベースに対してはクライアントとして振る舞います）。このアプローチはしばしば大規模なアプリケーションを機能の領域ごとに小さなサービスに分割する際に用いられるもので、あるサービスが他のサービスの機能やデータを必要とする場合にリクエストを発行することになります。こういったアプリケーションの構築方法は、従来サービス指向アーキテクチャ（service oriented architecture、SOA）と呼ばれていましたが、最近では洗練され、マイクロサービスアーキテクチャと呼ばれます 131,32」。
いくつかの面で、サービスはデータベースに似ています。これらは通常、クライアントからのデータの投入とクエリを受け付けます。とはいえ、データベースでは 2 章で論じたようなクエリ言語で任意のクエリが使えるのに対して、サービスが公開するのはアプリケーション固有の API であり、許される入出力はそのサービスのビジネスロジック（アプリケーションコード）が事前に規定しているものだけです（33）。この制約によって、ある程度のカプセル化ができるようになります。
サービスは、クライアントが行えることと行えないことについて、精密に制約をかけることができます。
サービス指向/マイクロサービスアーキテクチャにおいて鍵となる設計目標は、それぞれのサービスを独立してデプロイし、進化させられるようにすることで、アプリケーションの変更とメンテナンスを容易にすることです。たとえば、それぞれのサービスは 1 つのチームが所有すべきであり、そのチームは新しいパージョンのサービスを他のチームとの調整なしに頻繁にリリースできるようになるべきです。言い換えれば、新田バージョンのサーバーとクライアントが混在して助することが予想されることから、サーバーとクライアントが使用するデータのエンコーディングはサービス、A1 のバージョン間で正換性を保たなければなりません。これはまさに本茶で通してもたことそのものです。
4.2.2.1Web サービス
サービス間のやりとりを支えるプロトコルとして HTTP が使われている場合、そのサービスは Web サービスと呼ばれます。Web サービスは Web でだけ使われるものではなく、以下のように様々な状況下で使われるものなので、これはおそらく誤解を招く呼び名でしょう。 1.ユーザーのデバイス上で動作しているクライアントアプリケーション（たとえばモバイルデバイス上のネイティブアプリケーションや Ajax を使う JavaScript の Web アプリケーション）
が HTTP 経由でサービスにリクエストを発行するようなケース。通常こういったリクエストは、公開のインターネットを経由します。
2．同一の組織内で、あるサービスが他のサービスに対してリクエストを発行するような場合。これは多くの場合、同一データセンター内でサービス指向/マイクロサービスアーキテクチャの一部として行われます（この種のユースケースをサポートするソフトウェアはミドルウェアと呼ばれることがあります）。 3. 他の組織が所有しているサービスに対してあるサービスがリクエストを発行する場合。これは通常、インターネットを経由して行われます。これは異なる組織のバックエンドシステム間でのデータ交換に使われます。これには、たとえばクレジットカード処理のシステムや、ユーサーデータへの共有アクセスのための OAuth などのオンラインサービスが提供する公開 API
が含まれます。
web サービスに関して広く使われるアプローチには、REST と SOAP という 2 つがあります。ごれらはその哲学という点ではほぼ完全に相反しており、それぞれの支持者の間でしばしば熱い診の題材となります”？。
RBST はプロトコルではなく、むしろ HTTP の原理の上に構築された設計哲学です 134.35。
REST で望されているのはリソースの説別に URL を使うシンプルなデータフォーマットと、キャッシュの前物、肥師、コンテントタイプのネゴシエーションに HTTP の機能を使うことでサロ ST は、少なくとも組間のサービス合という観点では SOAP よりも広く使われるよう
「13、ばしばマイクロサービスと関送づけられます 11。REST の原理によっ
いて設計された API は RESTfuI と呼ばれます。
これに対し、SOAP はネットワーク API のリクエストを発行するための XML ペースのプロトルですす 8。HTTP 経由で使われることが最も一般的であるとはいえ、SOAP は HTTP から独立を保ち、HTTP の機能をほは使わないことを目標としています。その代わりに、SOAP には様々な機能を追加する、広範囲にわたる複雑な関連標準が多数あります（web serice frameworkeWS"と
呼ばれます）137」。
SOAP Web サービスの API は、WSDL（Web Service Description Language）と呼ばれる
XML ベースの言語で記述されます。WSDL を使うことでコード生成が可能になり、クライアントはローカルのクラスやメソッド呼び出し（これは XML のメッセージにエンコードされ、後に再度フレームワークによってデコードされます）を使ってリモートのサービスにアクセスできます。これは静的型付き言語では有益ですが、動的型付き言語の場合はそれほど役には立ちません （p.136
「4.1.4.5 コード生成と動的型付き言語」参照）。
WSDL は人間が読めるようには設計されておらず、SOAP メッセージは手作業で構築するには複雑すぎることが多いので、SOAP のユーザーはツールによるサポート、コード生成、IDE に強く依存することになります 138）。SOAP ベンダーにサポートされていないプログラミング言語のユーザーには、SOAP サービスの結合は難しいでしょう。
表面的には SOAP とその様々な拡張は標準化されていますが、様々なベンダーの実装間での相互運用性は、しばしば問題を引き起こしています［39］。以上のような理由から、SOAP は依然として多くの大企業で使われているものの、ほとんどの小規模な会社においては支持されなくなっています。
RESTful API ではシンプルなアプローチが好まれる傾向があり、通常はコード生成や自動化されたツールの必要性が低くなっています。OpenAPI のような API 定義のフォーマット（Swagger
とも呼ばれます［40］）を使えば、RESTful な API を記述し、API のドキュメンテーションを生成することもできます。
4.2.2.2
：リモートプロシージャコール（RPC）の問題
Web サービスは、ネットワーク越しに API リクエストを発行する技術の長い歴史における最新の技術に過ぎません。それらの技術の多くは熱狂を受けたものの、重大な問題を抱えていました。 Enterprise JavaBeans （EJB）や Java の Remote Method Invocation （RMI）は Java に限定されたものでした。分散コンポーネントオブジェクトモデル （Distributed Component Objectdel、 DCOM）はマイクロソフトのプラットフォームに限定されていました。 Comion Object
Request Broker Architecture （CORBA）はあまりに複雑であり、後方及び前方互換性を提供していませんでした 141。
これらはすべて、1970 年代から存在するリモートプロシージャコール（RPC）の概念に基づいています（42）。RPC モデルは、リモートにあるネットワークサービスへのリクエストの発行を、同＝プロセス内でのプログラミング言における関数やメソッドので出しと同じように見せようとします（この抽象化は場所の透過性と呼ばれます）。RPC は一見使利なように見えるものの、このアプローチには根本的な問題があります（13.14）。ネットワーク越しのリクエストは、ローカルの関数呼び出しとは大きく異なるのです。
ローカルの関数呼で出しは子週が可能であり、自分で制細できるパラメータにのみ依存して成功か失敗かが決まります。ネットワークのリクエストは予測が不可能です。リクエストやレスポンスがネットワークの問題で消失したり、リモートマシンが低速であったり利用不可能な状態にあったりします。そしてこういった問題は、利用者が制できるものではありません。ネットワークの問題は一般的なものなので、それらを予測し、たとえばリクエストが失敗したらリトライするといったことが必要です。
ローカルの関数呼び出しは、結果を返すか、例外を投げるか、制を返してこない（無限ループに入ったりプロセスがクラッシュしたりといった理由で）かのいずれかです。ネットワークリクエストには、もう 1 つの終わり方があります。すなわち、タイムアウトしたために結果を返さずに制倒が戻るというケースです。この場合、単純に何が起きたのかは分からないことになります。リモートのサービスから結果を受け取れなかったのであれば、リクエストが到達していたかどうかを知る方法もありません（この問題については 8 章で詳しく論
じます）。
• 失敗したネットワークのリクエストをリトライした場合、1 つめのリクエストが実際には到達していて、レスポンスだけが失われていたということがあり得ます。この場合、プロトコルに重複排除の仕組み（冪等性、idempotence）を構築しないかぎり、リトライをすることによってアクションが複数回実行されてしまうことになります。ローカルでの関数呼び出しにはこの問題はありません（冪等性については 11 章で詳しく論じます）。
ローカル関数の呼び出しの場合、実行にかかる時間は毎回ほぼ同じです。ネットワークリクエストは関数呼び出しよりもはるかに低速であり、そのレイテンシの変動幅は大きくなります。いいときには 1 ミリ秒以下で完了したものが、ネットワークが輪験していたり、リモートサービスが過負荷に陥っていたりすれば、まったく同じ処理をするのに数秒を要するかも
しれません。
•ローカル関数を呼び出す場合には、ローカルメモリ中にあるオブジェクトへの参照（ポイン
3）を渡すことができるので効率的です。ネットワークリクエストの場合、そういったバラメータはすべてバイトの並びにエンコードして、ネットワークを通じて送れるようにしなけはなりません。パラメータが数値や文字列のようなプリミティプであればこれは問題になりませんが、オブジェクトが大きくなってくればすぐに問題になります。
•A アイントとサーバーは異なるプロクラミング言で実装されているかもしれないので、「のフレームワークは 1 つの言舗から他の言ヘデータ型を必換しなければなりません。
すべての直話が同じ製を持っているわけではないので、これはひといことになるかもしれない
。たとえば 23 よりも大きい数値に関する JawaScint の問題を思い出してください。
この問題は、単一の言語で書かれている単一のプロセスには存在しません。
こういった要素すべてが示しているのは、リモートサービスと使用しているプログラミング言語そのローカルオブジェクトは根本的に異なっているので、これらを同じものに見せようとしすぎるのは意味をなさないということです。REST の魅力の一部は、それ自身がネットワークプロトコルであるという事実を隠そうとしていないことにあります（とはいえ、そのことをもってしても人々が REST 上に RPC のライプラリを構築しようとすることは止められないようです）。
4.2.2.3
現在の RPC の方向性
こうした問題があるにもかかわらず、RPC はなくなってはいません。本書で取り上げたあらゆるエンコーディングの上で、様々な RPC フレームワークが構築されてきました。たとえば Thrift
や AITO には RPC のサポートが含まれており、Protocol Buffers を使った RPC の実装としては BRPC があり、Finagle は Thrint を、Rest.l は HTTP 上で JSON を使っています。
新世代の RPC フレームワーク群は、リモートリクエストがローカルの関数呼び出しとは異なることを明確にするようになってきました。たとえば Finagle や Rest.li は future （promise）を使って、失敗するかもしれない非同期の動作をカプセル化します。future は複数のサービスに対して並列にリクエストを発行しなければならない状況を単純化し、それらの結果をまとめてくれます 145）。gRPC はストリームをサポートしており、単一のリクエストやレスポンスだけではなく、時間軸上のリクエストとレスポンスの列から構成されるものも 1 回の呼び出しで扱えます［46］。
これらのフレームワークの中にはサービスディスカバリを提供しているものもあります。これは、クライアントが特定のサービスを見つけるための IP アドレスとポート番号を知ることができるものです。この話題は p.231「6.5 リクエストのルーティング」でもう一度論じます。
バイナリのエンコーディングフォーマットを使うカスタムの RPC プロトコルは、REST 上の JSON のように汎用的なものよりも優れたパフォーマンスを発揮できます。とはいえ、RESTful
な API にはパフォーマンス以外の大きな利点としては、実験とデバッグに適していること（Web
プラウザや curL のようなコマンドラインツールさえあればリクエストを発行でき、コード生成や何かソフトウェアをインストールする必要がありません）、主要なプログラミング言語やブラットフォームのすべてがサポートしていること、ツールの巨大なエコシステム（サーバー、キャッシュ、ロードバランサ、プロキシ、ファイアウォール、モニタリング、デバッギングツール、テストツールなど）があることがあります。
こういった理由から、REST は公開 API のスタイルとしては支配的になっているように見えます。RPC フレームワークの焦点は同一組織内のサービス間のリクエストとなっており、この場合のやりとりは同じデータセンター内で行われることが普通です。

4.2.2.4 RPC のデータエンコーディングと進化
選化性という点では、RPC クライアントとサーバーに変化を加えることができ、独立してデッロイを行えることが要です。データベースを流れるデータをこれについては前セクションでもべました）と比較すれば、サービス間のデータフローのケーズは 1 つの仮定を置いてシンプルにすることができます。すなわち、すべてのサーパーがまずアップデートされ。クライアントはその後にアップデートされると考えてもよいでしょう。したがって、保たなければならないのは、リクエストの後方互換性とレスポンスの前方互換性のみです。
RPC のスキームにおける後方及び前方互換性の特性は、その RPC が使うエンコーディングから引き継がれます。
• Thift、 gRPC （Protocol Buffers）、AVo RPC は、それぞれエンコーディングフォーマットの互換性のルールに従って進化させることができます。
• SOAP では、リクエストとレスポンスは XML スキーマとあわせて指定されます。このスキーマは進化させることができますが、見落としがちな落とし穴もあります 147）。
RESTful API では、レスポンスに JSON（正式なスキーマ指定なしで）を、リクエストには JSON あるいは URI エンコード/form エンコードされたリクエストパラメータを使うことが一般的です。通常、オプションのリクエストパラメータの追加やレスポンスオブジェクトへの新しいフィールドの追加は互換性を保つ変更と考えられます。
サービスの互換性が難しくなるのは、組織間の境界をまたぐ通信に RPC が使われることからサービスのプロバイダがクライアントを制御できず、アップグレードを強制できないためです。したがって互換性は長期にわたって保たれなければならず、おそらくはその期限を切ることもできません。互換性が保たれないような変更が必要になった場合、サービスプロバイダは複数バージョンのサービス API を並行してメンテナンスしていかなければならなくなります。
API のバージョン管理をどのようにするべきか（たとえば使用したい API のバージョンをクライアントが示す方法（28）ということについては、合意が成されていません。RESTfu API の場合、URL 内か HTTP 上の Accept ヘッダ内でバージョン番号を使うのが一般的なアプローチです。
クライアントの説別に API キーを使うサービスの場合、クライアントが要求した API のバージョンをサーバーに保答しておき、独立した管理用のインターフェースを通じて選択されたパージョンを更新できるようにしておくという選択肢もあります 149）。
4.2.3
メッセージパッシングによるデータフロー
こまで下されたデータがあるプロセスから他のプロセスに流れる様々な方法を見てきました。
らまえでいてきたのは REST と RPC（あるプロセスがネットワークを通じて他のプロセスにりクエストを受けると、できる限り素早くレスポンスが返されることが期待されるような場合）とデータベース（あるプロセスがエンコードされたデータを書き、他のプロセスがそれを特来のどこかの時点で読み直すような場合）についてでした。
この最後のセクションでは、非同期のメッセージパッシングシステムについて簡単に見ていきましょう。これは RPC とデータベースの中間のどこかに位置するもので、クライアントのリクエスト（通常はメッセージと呼ばれます）が他のプロセスに低いレイテンシで配送されるという点では RPC に似ています。一方で、そのメッセージが直接のネットワーク接続を通じて送信されるわけではなく、一時的にメッセージを保存するメッセージブローカーと呼ばれる中継地点（メッセージ
キューあるいはメッセージ試行ミドルウェアと呼ばれることもあります）を経由するという点ではデータベースに似ています。
直接の RPC と比較して、メッセージブローカーの利用にはいくつかの利点があります。
・受信側が動作していなかったり過負荷に陥っていたりする場合に、メッセージブローカーはバッファとして働くので、システムの信頼性が向上します。
・ メッセージブローカーは、受信側のプロセスがクラッシュした場合にメッセージを再配することによって、メッセージが失われてしまうことを避けられます。
• メッセージブローカーを使うことで、送信側は受信側の IP アドレスやポート番号を知る必要がなくなります（これは、仮想マシンが入れ替わるクラウド環境で特に有益です）。
• メッセージブローカーを使うことで、1 つのメッセージを複数の受側に送できます。
● メッセージブローカーによって送信側と受側が論理的に分離されます（送側は単にメッセージを発行するだけであり、そのメッセージの受信者のことは関知しません）。
ただし RPC と比較した際の違いとしては、通常メッセージパッシングによるコミュニケーションは単方向になるということがあります。すなわち、送信側はメッセージに対するリプライを受情しないものと考えるのが普通です。プロセスからレスポンスを送信することはできますが、それは別個のチャネルを通じて行われます。このコミュニケーションパターンは非同期です。送信側はメッセージが配されるのを待たず、送信をしたらそのメッセージのことは忘れてしまうのです。
4.2.3.1 メッセージブローカー
過去には、メッセージプローカーは TIBCO、IBM Websphere, webMethods といった企業の商用エンタープライズソフトウェアによって占められていました。最近では、RabbitMQ、AoiveMQ, HornetQ. NATS、 Apache Kalka といったオープンソースの実装の人気が高まっています。これらについては 11 章で詳しく比較します。
詳細な配信のセマンティクスは実装や設定によって異なりますが、概してメッセージプローカーは水のように利用されます。まずあるプロセスが名前付きのキュー、あるいはトピックに対してメッセージを送信します。そしてブローカーは、キューもしくはトピックの 1 つ以上のコンシューマもしくはサブスクライバにそのメッセージが届いたことを保証します。1 つのトピックには、多くのプロデューサやコンシューマが関わることがあります。
パピックが飲するのは、一方通行のデータフローのみですんではアクテンシューマ自体も他のトピックへメッセージを展開したり（1 年で見るようにっこれで ♪ ビックを連鎖させることができます）、もともとのメッセージの送観が散するリプライキエラクンセージを展開したりすることはできます（RPC に似たリクエスト/レスポンス型のデータフローが実現できることになり
ます）。
通常メッセージブローカーは、特定のデータモデルの利用を強制しません。メッセージは、多少のメタデータを持つバイト列に過ぎないので、任意のエンコーディンクフォーマットが利用できます。エンコーディングが後方及び前方互換性を持つなら、高い柔戦性を持ってパブリッシャとコンシューマを任意の順序でそれぞれ独立に変更してデプロイできます。
コンシューマがメッセージを他のトピックへ再発行する場合は、先にデータベースの場合について述べたような問題が生じないよう、未知のフィールドの保全に注意を払わなければならないかもしれません（図 4-7）。
4.2.3.2 分散アクターフレームワーク
アクターモデルは、単一プロセス内での並行処理プログラミングモデルです。このモデルではスレッド（及びスレッドにまつわるレース条件、ロック、デッドロックの問題）を直接扱うことはせす、ロジックをアクターにカプセル化します。通常、それぞれのアクターは 1 つのクライアントないしはエンティティを表現し、何らかのローカルな状態を持つことがあり（この状態は他のアクターと共有されません）、非同期なメッセージの送受言によって他のアクターと通信します。メツセージが配信されることは保証されません。ある種のエラーの状況下では、メッセージは失われてしまいます。それぞれのアクターは、ある時点では 1 つのメッセージしか処理しないのでスレッドに関する配は不要であり、アクターのスケジューリングはアクターごとに独立にフレームワークが処理します。
分散アクターフレームワークでは、アプリケーションを複数ノードにわたってスケールさせるためにこのプログラミングモデルが用いられます。使われるメッセージパッシングの仕組みは同じであり、送信側と受信側が同じノードにあるか異なるノードにあるかは問題になりません。それらが異なるノードにある場合、メッセージは透過的にバイト列にエンコードされ、ネットワーク離由で送され、他方でデコードされます。
マッターモアルでは、RPC よりも場所の透過性がうまく働きます。これは、アクターモデルではメンセージがリープロセス内でさえ失われうることを前提としているためです。同一プロセス内
2 はコカルとリネートワーク越しの場合レイテンシは大きくなりますが、アクターモデルを用いるとローカルとリモートのコミュニケーションの基本的なミスマッチは少なくなります。
クルアクターフレームワークは、基本的にメッセージフローカーとアクタープログラミングモす
ルンルコのフレームワークに統合したものです。とはいえ、彼にアクターベースのアプリケージもこのローリングアップグレードを行いたいような場合には、やはり前方及び後方互換性を気にする必要があります。これは、新しいバージョンが作しているノードから古いバージョンが動作しているノードへメッセージが送られたり、その逆があったりするためです。
ディングの扱いを紹介します。
以下に、広く利用されている 3 つの分散アクターフレームワークにおけるメッセージのエンコー
• Akka は、デフォルトで Java に組み込まれているシリアライゼーションを利用します。これは前方あるいは後方互換性を提供しません。しかし、このシリアライゼーションは ProtocolBufers のようなもので置き換えることができ、そうすればローリングアップグレードも可能になります［50］。
Oreans は、デフォルトでカスタムのデータエンコーディングフォーマットを使いますが、これはローリングアップグレードでのデプロイをサポートしていません。新バージョンのアプリケーションをデプロイするには、新しいクラスタを立ち上げ、トラフィックを古いクラスタから新しいクラスタへ移し、古いクラスタをシャットダウンするという手順を踏まなければなりません 151,52］。 Akka と同じく、シリアライゼーションのプラグインを使うこともできます。
Erlang OTP では、レコードのスキーマに変更を加えることは驚くほど難しいことです（このシステムが高可用性のために設計された多くの機能を持っているとはいえ）。ローリングアップグレードは可能ですが、注意深く計画することが必要です［53。実験的な新データ型である maps （JSON に似た構造で、2014 年に Erlang R17 で導入されました）によって、将来的にはローリングアップグレードが容易になるかもしれません「54）。
まとめ
本章では、データ構造をネットワーク上やディスク上のバイト列に変換する方法を見ました。こうしたエンコーディングの細部は効率性に影響するだけではなく、さらに重要なことにはアプリケーションのアーキテクチャやアプリケーションのデプロイの選択肢にまで影響します。
特に、多くのサービスではサービスの新バージョンをすべてのノードに同時にデプロイするのではなく、1 回に少数のノードずつ、徐々にデプロイしていくローリングアップグレードをサポートする必要があります。ローリングアップグレードができるなら、サービスの新バージョンをダウンタイムなしで（したがって大きなリリースをまれに行うのでなく、小さなリリースを頻楽に重ねやすくなります）、小さなリスクでデプロイできます（リリースに問題があれば検出し、それが多くのユーザーに影響する前にロールバックできるようになります）。こうした特性は、アプリケーションへの変更の加えやすさである進化性にとってきわめて有益です。
ローリングアップグレードの間、あるいはその他の様々な理由のために、アプリケーションのコードの様々なパージョンが様々なノードで混在して助作するものと考えなければなりません。したがって、システム内を流れるすべてのデータは後方互換性（新しいコードが古いデータを読め）と舗方正発性（缶いコードが新しいデータを読める）が採されるような方法でエンコードされ
なければなりません。
本体ではデータエンコーディングフォーマットと、それらの互換性に関する特性を高じました。
・ブログラミング乳間 4 のエンコーディングは 1 つのブログラミング音器に限定され、しょ
しば前方及び後方互換性を父きます。
•JON、XM、CSV といったテキストフォーマットは広く利用されており、その互換性は利用の方法に依存します。これらにはオプションのスキーマ言話がありますが、それらは助けになることもあればむしろ障害になることもあります。これらのフォーマットはデータ型について多少の腰味さがあるので、数値やバイナリ文字列などについては注意が必要です。
• Thrift、Protocol Buffers、 Avo といったスキーマを持つバイナリフォーマットではコンパクトで効率的なエンコーディングが可能であり、前方及び後方互換性のセマンティクスも明確に定義されています。これらのスキーマは、ドキュメンテーションと静的型付き言語でのコード生成に役立ちます。ただし、バイナリフォーマットにはデコードしなければ人にはデータが読めないという欠点もあります。
データフローの形態についても論じました。その中で、データのエンコーディングが重要になる様々な状況を紹介しました。
• データベースでは、データベースへの書き込みを行うプロセスがデータをエンコードし
データベースからの読み取りを行うプロセスがそのデータをデコードします。
• RPC と REST API では、クライアントがリクエストをエンコードし、サーバーはそのリクエストをデコードしてレスポンスをエンコードします。そして最後にクライアントがレスポンスをデコードします。
、非同期のメッセージパッシング（メッセージプローカーあるいはアクターが使われます）では、ノードはお互いにメッセージを送信することによって通信し、送倍側がメッセージをエンコードし、受信側がそのメッセージをデコードします。
まとめると、多少の注意を払うことで前方/後方互換性やローリングアップグレードは十分に実現可能です。素早いアプリケーションの進化と、頻繁なデプロイを目指しましょう。
