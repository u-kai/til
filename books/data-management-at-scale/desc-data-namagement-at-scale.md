---
marp: true
---

# 大規模データ管理に関するレポート

---

# 目次

- はじめに
- 本レポートの目的
- 大規模データ管理本の概要

---

# はじめに

- 今回は本書籍から得られた技術的観点にフォーカスを当てて紹介をします
- データ活用基盤は技術観点とデータ基盤がなぜ必要なのかという目的ベースの 2 つが必須ですが今回は技術観点の話です
- 本の内容自体が抽象的であったため，具体的な実装事例などは登場しません
  - あくまで実装の考え方を示すものになります

---

# 本レポートの目的

- 大規模データ管理で紹介されていた考え方について共有し，今後の技術的選択に寄与する
- IT 推進部として関わるところと部門に委任する境界に関する考え方を提示する

---

# 大規模データ管理本の概要

- 筆者は大企業のデータアーキテクチャ計画を推進した際の経験に基づいている

  - 直近十年の間,企業がデータ駆動になるための完璧なソリューションを探し続けていた

- データ管理と統合に関する先進的なアプローチが紹介されている
- 筆者が思い描いているのは長期的に使い続けられるような最新の**分散型ドメインベースアーキテクチャ**

## この本ではこのアーキテクチャを Scaled Architecture と呼ぶ

---

# データウェアハウス，データレイクとの違い

- 分散型で,ドメインベースあること

  - ドメインとは，ビジネスの問題領域のこと
    - 例えば, 顧客情報, 製品, 販売, 物流,財務など

- 分散型なので他のドメインとは疎結合が望ましい
- 緩やかな統合なので小さく始められる
  - というか小さく始めるべき!!

---

# 第 1 章　データ管理の崩壊

## 現在機能している中央集権的なソリューションは将来的にうまく機能しなくなる

---

# 理由

- ソフトウェアの提供速度とともにデータの複雑度も増している

  - システムの負荷や，プライバシーや法規制についても考える必要が出てきた
  - データの管理方法が重要

- データレイクやデータウェアハウスはデータ消費の要求が急速に高まることは想定してない
- 中央集権モデルは破壊的なトレンドによって失敗する可能性がある

---

## 1 章 8 節 企業を悩ます時代遅れのデータアーキテクチャ

- 企業のデータ統一は非常に複雑なプロセスで完了するまでに何年もかかる

- データの意味はドメインや部門，システムごとに異なる場合が多い
- データが増えれば増えるほど，また定義の矛盾や不整合が生じれば生じるほど調整が難しくなる

### コンテキストを統一しても誰にとっても意味のないものになってしまう!

### コンテキストが正しくなければデータの価値はない

---

# データウェアハウス

### 前提

今回のデータウェアハウスとは,RDB などを使い，全てのデータを構造化して統合するソリューションのこと

### 欠点

- 結合による多くの依存関係
- アジリティの欠如
  - アジリティが欠如すると人は抜け道を探し出し，それが技術的負債になる
- データの流通や消費に関する情報は欠如
  - **データの所有権や出所もわからなくなる**

---

# データレイク

### 前提

非構造データなど，ほとんど全てのデータを保存することができる集約点のこと

Garther のアナリスト Nick Heudecker はデータレイク導入の失敗率は 6 割を超えているとツイートしている

### 欠点

- 生データを提供するので迅速さはあるが，常にデータの修正が必要になってくる
- データのコンテキストを整えるために，他のデータと組み合わせないといけない
- 生データはいつでも変化する可能性があり，実験やユースケースを本番環境に導入できない
- 何度も何度も変換作業を繰り返す必要がある

---

# Scaled Architecture

- 複雑にサイロ化したデータ問題を解決するのが Scaled Architecture
- ドメインベースの参照アーキテクチャ

---

# 2 章

- き業がデータ駆動方になるには?，アジリティ，セキュリティ，コントールが実現できるような形でデータを効率的に流通させるには？
- これらの疑問について取り上げる章

- アプリケーションごとにアプリケーションデータベースを持つ
- アプリケーションは特化したものであり、一意のコンテキストを持つ
- ゴールデンソース
  - データがどこからきたのか？どこで管理されているのかを知ることが重要
  - ゴールデンソースとは信頼できるデータが,ある特定のコンテキストで管理されている
  - 1 つまたは複数のゴールデンデータセットで構成される
- ゴールデンデータセット
  - 生成された権威あるオリジナルデータのこと
  - 真正、かつ一意なもので，性格で完全でよく知られていなければいけない
  - 人間が読むことのできる情報である[データ要素]で構成
  - データ要素には識別可能な名前がついており，データガバナンスなど他データ管理主体との接着剤の役割を果たす

## ゴールデンソースとゴールデンデータセットがあればデータに対して正確で一貫性のある説明を行うことができる

---

# データ統合のジレンマから逃れられない

- データをあるアプリケーションからあるアプリケーションへ統合，移動させる時はその際の一意のコンテキストがある
- そのコンテキストを反映するデータ変更が必要なため常にデータ変換は必要で，つまり新しいアーキテクチャを常に形作ることになる

---

# DDD(ドメイン駆動設計)

- ドメインとは取り組もうとしている問題空間
- コンテキスト境界
- ユビキタス言語
- エンタープライズアーキテクチャを成功させるためには優れたビジネスアーキテクチャの設計が不可欠

  - ビジネスアーキテクチャとは,機能、エンドツーエンドの価値提供、情報、組織構造等を表すもの
  - ビジネスケイパビリティ
    - 特定のコンテキストにおけるデータ，プロセス，組織，テクノロジー関係をまとめて現したもの

- すべての戦略的ビジネス目標をビジネスケイパビリティにマッピングし，ビジネスケイパビリティとバリューストリームにまとめる
  - バリューストリームを用いることで，ビジネスにおいて，外部や内部のステークホルダーが組織から得ることのできる価値の流れが明らかになる
- ビジネスケイパビリティは組織上の変更に依存しないようにすべき

  - 企業の目標を達成するために手段が変わろうが機能には変わりがないべき

- ビジネスケイパビリティは実装ではなく，解決すべき問題空間を対象にして抽象的にすべき
- これが実装されたものがケイパビリティインスタンス
- ビジネスケイパビリティとアプリケーションアーキテクチャの関係を明確にすることで多くのメリットが得られる
  - データとその所有者が決まる
- コンテキスト境界を意識すること

---

## 通信と統合のパターン

- ポイントツーポイント
  - アプリケーションから別のアプリケーションへ、直接通信を行うポイントツーポイント
  - アプリケーション間を密結合に
- サイロ型
  - 全てのデータとアプリケーションロジックをまとめてサイロを作る
  - いくつもの不整合を生み出し，統合作業が膨大となる
- ハブスポークモデル
  - 中央にデータ流通のためのハブを作成して、様々なシステムやアプリケーションのインターフェイスを接続
  - データ流通ハブやデータブローカーアーキテクチャー

---

- 企業が必要としてるのは、データプロパイダーとデータコンシューマを簡単に接続し、柔軟性、コントロール、インサイトを提供できる、スケーラブルで高度な分散型アーキテクチャ

- ゴールデンソースとドメインデータストア
- Scaled Architecture の最初の原則はゴールデンソースシステムとゴールデンデータセットの流通のみを許可
- 変更はゴールデンソースでしか行わない
- ゴールデンデータセットだけを提供する
- ゴールデンデータセットはアプリケーションのサブセットである
- ゴールデンデータセットと要素は検索性を高めるために中央に登録される

  - ゴールデンソースと所有権メタデータをツールやプラットフォームを通じて中央で利用できるようにすることが重要

- DDS(ドメインデータストア)
  - ゴールデンソースシステムとは異なり，他のシステムからデータのコンテキストを消費して統合し，変更するためのもの
  - DDS が新しいゴールデンデータセットを作成するとそれが新しいゴールデンソースになることがある
    - この場合はデータは全く新しいコンテキストをもち，新しい事実が生まれる
    - また新しい所有権がうまれる
      - 新しいデータは新しい所有権をうむ
      - 受け取ったゴールデンデータセットを転送することは許されない
      - 必要にない DDS は作成しない

---

## エンタープライズ規模でのドメイン駆動設計

- データ流通にデータウェアハウスのようなサイロを使わない
- コンテキスト境界ごとに分離することが大事
- 他のコンテキスト協会と通信する場合は必ずデータレイヤー(後述)を通して行う必要がある
- サイロ型はエンタープライズ言語を使って高度に統合された環境に中央集権化されている
- DDD ではコンテキスト協会ごとに独自のユビキタス言語が使われている

  - コンテキスト境界が変わればユビキタス言語も変わる
  - エンタープライズ言語は多くのユビキタス言語の集合体となる

- 原則

  - データはドメイン全体で管理／流通される
  - コンテキスト境界はインスタンス化されたビジネスケイパビリティに関係付けられる
  - コンテキスト境界は 1 つまたは複数のアプリケーションに関連づけることができる
  - ユビキタス言語はコンテキスト境界の中で共有される
  - コンテキスト境界はインフラストラクチャー，ネットワーク，組織から独立する
  - 一つのコンテキスト境界は一つのチームに属する
  - 境界線は厳密にする
  - 教会を越えるときは分離する
  - 教会内であればどんな形式でも問題ない
  - 仮想化と調和はコンテキスト境界の中でのみ許可される
  - データは付加システムや中間システムを介して配信されるべきではない
  - データレイヤーはドメインロジックを含むべきではない

- DDD アプローチではアジリティが大幅に向上する
  - データプロバイダーとデータコンシューがより疎結合になり，データがサイロで統合されるまで待つ必要がないから
  - ドメインが持つ依存関係はデータレイヤーのみになるため論理的なコンテキスト教会内に存在するビジネスドメインはそれぞれ独自の速度で変化できるようになる
- データプロバイダーとデータコンシューマーがそれぞれのコンテキスト境界から直接通信することで，他にもいくつかの興味深い結果が得られる

---

## 単一ステップの変換

- DDD モデルではプロバイダーのコンテキストからコンシューマーのコンテキストへの単一ステップの変換だけでよくなる
- 企業全体の単一の統合ソリューションを作る必要がなくなる
- 企業レベルのカノニカルモデルを持たなくなることで定義間の不整合や信頼できる情報に複数のバージョンが存在することを避けることができる
  - これらはデータウェアハウスで問題になっていたこと
  - これって，つまり，信頼できるプロバイダーがの存在をみんなが知っていればそこからデータをコンシュームするから，不整合や複数のバージョンが存在することを避けることができるってこと？
    - これって他のドメインがプロバイダーの存在やそのプロバイダーのドメインを知っているってこと？
    - この疑問がそのまま欠点に書いてあった

---

# 読み出しに最適化されたデータ

- あるコンテキストから別のコンテキストにデータを移動するには両方のコンテキストに関する知識が必要となるため常に困難を伴う
- あるコンテキスト教会から別のコンテキストに対してデータをどのように公開／提示するかについていくつかの原則を設定する必要がある

- 原則

  - アプリケーションの技術的な情報を隠す
    - 抽象的なインターフェイスにするべき
  - 集中的なデータ消費に最適化する
    - 正規化されたデータではなく，コンシューマが意味的に理解できるような形で提供すること
    - 正規化されたデータはコンテキスト内部にて扱うこと
  - ユビキタス言語は通信のための言語である
    - データプロバイダーとして機能するコンテキスト境界はそれぞれ独自のユビキタス言語を使ってデータを公開する必要があり，データプロバイダーは他のドメインのビジネスロジックを自分のドメインに組み込むべきではない
  - インターフェイスには一定の成熟度と安定性が求められる
  - データはすべてのパターンで一貫しているべき

- これらの原則から結論づけられるのはデータプロバイダーはより使いやすく，より消費しやすく，論理的な方法でデータを公開すべきということ
  - 生データにつきものの繰り返し作業は避けるべき
  - 論理ビジネスモデルの抽象版としてデータは表現されるべき

---

## 全体像としてのデータレイヤー

- データ変換を容易にすると同時に，ポイント二ポイントの複雑さを軽減し，サイロ型を回避できるような新しいモデルがデータレイヤー
- データレイヤーは高度に管理された安全なアーキテクチャ

  - ドメインが多種多様なパターンを使ってデータを流通できるようにする
  - ドメインはデータの消費と流通を実現するために簡単に自分自身を拡張することができる
  - また，データレイヤーはデータ流通の基盤となるインフラやプラットフォームを活用して，すべてのデータ管理要件が十分に満たされていることを確認する

- データレイヤーの実装は 3 つのアーキテクチャが密接に連携してさまざまなエンドポイントに対してデータを永続化，ルーティング，変換，操作，複製している
- この 3 つのアーキテクチャは完全にメタデータ駆動型
  - 再利用可能なデータサービスを定義するのに役にたつ
  - また，メタデータは流注のリネージや消費要件，データの品質，意味などについての情報も提供する
  - このメタデータを API 経由で公開することで，データプロバイダーとデータコンシューマーはさまざまなアーキテクチャとの相互運用性が高まり，データパイプラインの自動化を実現できるようになる
  - インターフェイスの複雑さを管理する上で大きな効果を発揮する
    - 実務担当者が多種多様なフォーマットに対応するのではなく，アーキテクチャが多種多様なデータモデルやフォーマットの使用を可能にすることで，多くの企業が必要とする柔軟性を提供する

---

# 読み出し専用データストアアーキテクチャ

- RDS(ReadOnly Data Store)

---

# API アーキテクチャ

- 書き込み，更新，削除処理を容易にする

---

# ストリーミングアーキテクチャ

- ストリーミングと API が異なるのは非同期であること，高いスループットを重視すること，そしてアプリケーションの状態をコピーするためにも使用できるようにすること

---

# メタデータとターゲット運用モデル

- 3 つのアーキテクチャは他の多くのアーキテクチャに比べてメタデータを多用する
- メタデータは内部のルーティングや流通の他，意味の把握やデータ完全性の検証にも使われる
- メタデータにより，統合機能を中央で提供するのではなく，アーキテクチャの一部や統合機能をドメインやチームが自ら構築，実行できるようにすることで，ターゲット運用モデルを採用することもできる
- メタデータを中央に保管することで，アプリケーションとアーキテクチャのランドスケープ全体のブループリントを作成することができる
- どのデータがデータレイヤーを通過するかがわかるためエンドツーエンドのリネージを可視化することができる
  - データリネージとはデータの作成から現在に至るまでの全履歴を追従可能にすること
- さまざまなドメイン間で激しい通信が行われている場合はコンテキスト境界が間違っていることがわかる
- サービス間の集中的なデータ交換は一般的にそれらのサービスが実際には単一のサービスであるべきことを意味する
- 流通しているデータを機械学習することで最適なデータスキーマが見つかるかも?
- このようにメタデータはアーキテクチャの全体の中で重要な役割を担っている

---

# 2 章まとめ

- 中央集権的なモノリスを取り除くことでドメインやチームが独立してデータを変更・交換できるようになる
- 各ドメインは全体アーキテクチャのある部分に対して責任を持つ
- 一度データはコンテキスト変換し、論理レイヤーに流す
- 単一の統合ソリューションやプラットフォームで全てのユースケースに対応できない
- 新しい統合アーキテクチャには企業全体を見る視点が必要

---

# 3

---

# RDS

- CQRS

### 3.2.2 大規模 CQRS

- アプリケーションモデルを、コマンドとクエリと呼ばれる、更新用と呼び出し用の 2 つのモデルに分割する。目的は、データ読み出しを集中的に行えるようにするため。
- 以下の原則に従う必要がある。

  - RDS は不変なデータベースであるため、読み出し専用。
    - 情報はゴールデンソースのアプリケーションでのみ修正できる
  - RDS には、権威あるオリジナルのゴールデンソースデータセットのみ配信出来る
  - データプロバイダーのコンテキストが RDS の設計を決定する
    - RDS の設計はデータプロバイダーのドメインで決定される、ということ
  - RDS はアプリケーション専用。他のアプリケーションと直接共有したり、依存関係を持つことは許されない.
    - 境界は厳格にしなければならない
  - データコンシューマは、RDS に直接新しいデータを作成することは出来ない。
    - ?
  - データプロバイダーは、データ品質、設計、RDS への配信に責任を負う

- 新しいアーキテクチャは、全てのデータ変換とデータ移動に対して必要な情報を与えるので、データプロバイダーからデータコンシューマへのデータ変換が容易
  - ?今後説明がある? メタデータのこと?
- ドメインとは関係ない基本的なコンポーネントのセットは、中央で提供すべき。
  - 中央で制御しても問題ない部分は制御し、メタデータやリネージ、データ品質、セキュリティを守りやすくする
  - ここは IT が価値を出せるところ

### 3.3.1 メタデータ

- まず考慮すべきなのは、企業のデータがガバナンス基準を満たすように共有プラットフォームを設計すること

  - プラットフォームを使ったら自動でメタデータを収集出来たり、データ品質チェックできたりするように設計するということ?

- 1.データプロバイダー
  - スキーマデータ、データ品質メトリクス、どのような参照データが利用されているかを直接取得できる
- 2.変換とリネージ
  - ETL、変換時に自動でメタデータを抽出する
    - どのようなデータからどのようなデータに変化したか、とか?
- 3.ETL されたあとのデータを読み込み用 RDS へ
  - データ配信契約に照らし合わせて、基準を満たしていない場合は拒否する
- 4. 読み込み RDS にあるデータの正確性、完全性、一貫性などのデータ品質を読み出す
- 5. ETL ツールなどから、データに何が起きてどこに移動したかを収集
- 6. データコンシューマ側でも 1 と同じデータを取得できる

### 3.3.14 分散型オーケストレーション

- データパイプラインの構築と自動化
  - データの抽出や、データプレパレーション、データの変換などの一連のステップを何度も繰り返す処理のこと
  - ツール群
    - Apache Airflow や Data Build Tool でオーケストレーションするのが人気
    - Apache Airflow では、DAG と呼ばれるタスクの依存関係を定義するための Python コードを書く
    - それによってデータパイプラインの構築が可能

## 3.7 まとめ

- RDS は、膨大な量のデータをデータコンシュマーが利用できるようにするためのもの
- RDS は単なるデータレイクやデータウェアハウスではなく、既存のアプリケーションを拡張し、そのコンテキストを継承するもの
- RDS の共通インフラストラクチャーと機能は中央から展開するのがおすすめ
  - ドメインで別個にホストするのではなく.

### 4.2.4 パブリックサービスとプライベートサービス

- パブリックサービスはビジネス機能を提供するもの
  - アプリケーションの境界を超えて動作する
- プライベートサービスは内部ロジックの一部として使用され，データベースのテーブルを公開するなど，技術的なデータやインフラストラクチャー機能を提供する
  - ビジネス上の問題とは無関係で密結合しがち
- プライベートサービスは通常組織全体で標準化されており，認証や認可，監視，ログ，デバッグ，監査など全てのドメインに適用される機能が含まれる

### 4.2.6.1 カノニカルモデルの大きさ

- 多くの組織では単一のカノニカルモデルを使用して，全てのサービスを記述しようとする
- これを行う場合，関係する全てのチームやパーティ間での調整が必要になり，サービスに大量のオプション属性がついてしまう
- その結果全ての人のニーズを満たすために妥協した，誰にも特化していないモンスターインターフェイスが出来上がる
- あまりにも汎用的なもので，サービスが正確に何を提供しているのか，何をしているのかが誰にもわからない

### 4.3.1 API ゲートウェイ

- ESB での統合ではなく，RESTful API を使った形になってきた
  - 誰かが統合するのではなく，お互いが宣言的に呼び出し合うって感じかな？
  - 逆に ESB ではどういう通信でやり取りしてた？
    - 同じサーバー内でプロセス呼び出しする感じか？
- API ゲートウェイはアダプターのオーバーヘッドや ESB の複雑な統合機能を持ち合わせないが，カプセル化を可能にし，API の利用状況をコントロール，保護，管理，レポートする管理機能を提供する

### 4.3.1 API ゲートウェイ

- ESB での統合ではなく，RESTful API を使った形になってきた
  - 誰かが統合するのではなく，お互いが宣言的に呼び出し合うって感じかな？
  - 逆に ESB ではどういう通信でやり取りしてた？
    - 同じサーバー内でプロセス呼び出しする感じか？
- API ゲートウェイはアダプターのオーバーヘッドや ESB の複雑な統合機能を持ち合わせないが，カプセル化を可能にし，API の利用状況をコントロール，保護，管理，レポートする管理機能を提供する

---

- 非中央集権的なモデルでは，ドメインは他のドメインに邪魔されることなく，それぞれのスピードで進化できる
- この新しいモデルではビジネスロジックやプロセス実行，データの永続化，コンテキスト検証を行う責任もドメインが持つようになる
- サービスに対する考え方として，いくつかの原則を追加できる

  - 複雑なシステムではなく，REST リソースモデルを使ってビジネス機能を公開すること
    - これにはビジネスコンセプトやそのプロパティを理解することや，関係に適切な名前をつけること，それに応じたスキーマの設計，適切な ID と一意性の確保などに集中する
  - 適切なレベルの粒度でサービスコンシューマ向けのサービスを構築すること
  - シンプルにして，最新のコミュニティ標準を使用すること
  - サービスオーケストレーションはドメイン境界外で行わないこと
  - 一貫性のあるドメイン識別子を使って，ドメインがリソース間の相互接続を識別できるようにすること

- このような原則に従うことで，API アーキテクチャをスケーラブルにすることができる
- ドメインは分離されたままで，作業を何度も繰り返すことなく他のサービスを利用することができる

---

### 4.3.4 サービス契約

- ドメインに責任を負わせることでサービスの作成と保守を中央チームからドメインに移管できる
- API 契約とは API がどのように設計されているのか(構造，プロトコル，バージョン，メソッドなど)を記録したドキュメント
- サービスプロバイダーとサービスコンシューマーの間で合意を形成するために利用できる
  - コンシューマーはプロバイダーに依存しているのでは？
  - 逆にプロバイダーはある程度コンシューマのことを気にしなくてもいいのでは？
- OpenAPI-Specifications(旧称 Swagger)を使うのが一般的

- コンシューマー駆動の話はマイクロサービスの本でも出てきたところ

---

### 4.3.5 サービスディスカバリ

- サービスを把握できるようにすべての API リストをサービスレジストリで管理することがおすすめ
- サービスレジストリとはすべての重要な API 情報を中央のレポジトリに保存するツール
- サービスレジストリにより，サービスの再利用が促進され，API の増殖を避けることができる
- サービスレジストリはすべてのサービスとどの API が誰によって公開されたり，消費されたりするのかを管理する
- 複数の API ゲートウェイを持つような連合型モデルでは，メタデータと API 登録が特に重要になる
- またサービスディスカバリ機能が提供されるというメリットもある

  - API のネットワーク位置や，稼働状態，バージョン番号，コンシューマの数などを管理できる
    - Netflix Eureka が例
  - サービスレジストリは，複数のクラウドベンダーと多くの API ゲートウェイが存在する分散環境では特に素晴らしいツールになる

- Consol ってのがいいってマイクロサービスの本ではいっていた
- HashiCorp の時代なのか？
  - Terraform
  - Consol
  - Vault

---

## 4.4 マイクロサービス

- マイクロサービスの特徴
  - サービスごとに独自のランタイムやライブラリ，ツール，機能を持たせることができる
  - 軽量な実行環境であるコンテナを利用するコンテナ化は配備の方法として人気がある
  - 各マイクロサービスでは通常，専用のデータベースにデータを保存する
    - 使用するデータベースもマイクロサービスごとに異なることがある
  - サービスは論理的に構成され，コンテキスト境界を中心に管理される
  - 他のマイクロサービスとの通信は軽量なもので行われる

### 4.4.1 マイクロサービスにおける API ゲートウェイの役割

- API ゲートウェイを使用することで他のドメインから隔離することができる
- マイクロサービスの本ではこれは非推奨とのことだった
  <tab>- ただし，それは通信点が増えるからとかで，それはしょうがなく，APIGateway が解決しようとしていることって,抽象的にすることなので，それ自体はひていされていない＋？

### 4.4.3 サービスメッシュ

- 大規模なマイクロサービスの通信を制御するためのパターン
- サービス間の通信を処理するための専用レイヤー(プロキシ)
- API ゲートウェイと少しにている
  ![](fig/4-11.png)
- 外部リーティングや認証，境界外からの受信トラフィックを API ゲートウェイが処理し，内部アプリケーションアーキテクチャのきめ細かい制御はサービスメッシュが担当する
  - なぜ細かい制御はサービスメッシュで，外部のトラフィックは API ゲートウェイなのかはよくわからん
    - メッシュを貼りまくるから？

## 4.7 メタデータ

- メタデータ管理によって，サービスコンシューマはサービスを検索できるようになり，再利用性が向上する
- 最初にすることは，属性や説明を含めすべての API を中央リポジトリで公開すること
- 特に複数の API ゲートウェイや EBS,サービスメッシュが導入されている分散環境では，すべての API を把握しておくことが重要
- 各統合機能がメタデータリポジトリに接続される場合にのみうまく動作する
  ![](fig/4-15.png)
- こうすることでどのサービスがどのドメインに属し，どのドメイン間が相互作用しているのかを知ることができる
- このような情報を持つことは全体像を管理する上で大きなメリットになる

# 5 イベントとレスポンスの管理：ストリーミングアーキテクチャ

ストリーミングアーキテクチャは 3 章の RDS アーキテクチャと 4 章の API アーキテクチャ両方の要素を含むため、もっとも複雑なアーキテクチャ

以下を紹介

- 非同期通信
- イベント駆動型アーキテクチャ
- Kafka などの OSS
- 一貫性モデル
- イベントタイプ

# 5.1 　ストリーミングアーキテクチャの紹介

- 様々なソースから生成されたデータを連続的にリアルタイムで処理・分析
- アプリケーションとシステムを非同期で接続
  - イベントプロデューサー（ソース）で発生したイベントをストリーミングアーキテクチャに入れる
  - ストリーミングアーキテクチャ内でイベントを集計・保存・関数を実行する
  - イベントコンシューマ（配信先アプリ）はストリーミングアーキテクチャからイベントを受け取る
- ストリーミングアーキテクチャ内にデータを永続化することも可能(RDS と似た部分)

# 5.2 　非同期イベントモデルがもたらすもの

ストリーミングアーキテクチャの場合、リクエストの方向がプッシュする流れになっている。

> イベントプロデューサー → ストリーミングアーキテクチャ → イベントコンシューマ

これは、API アーキテクチャのプルの流れとは逆。

> イベントプロデューサー ←API アーキテクチャ ← イベントコンシューマ

プッシュの流れは、データの流れとは同じなのだが、配信先アプリ(イベントコンシューマ)を触っている人からしたら違和感がある。例えば、リンクをクリックしたらすぐに新しいページが表示されることを期待しているはず。

人の行う作業の中で、プッシュする非同期通信に近いのは**電子メール**。送信ボタンを押したら相手にメールがプッシュされ、そのあとは送信側は確認しない。このような疎結合により、通信相手が反応するのを待つ必要がないので、依存関係もなくなる。

非同期データ通信の際はメッセージキューにイベントを保持することが多い。

最新のメッセージキューでは分散機能/フォールトトレランス機能/長期的な永続的ストレージのような機能を持つ（詳細は後述）

# 5.3 　イベント駆動型アーキテクチャとは

- イベント駆動型アーキテクチャ(EDA:event-driven architecures)
  - サービス駆動ではなく、イベント駆動を中心に考えたアーキテクチャ
  - イベント駆動型アーキテクチャでは
    - データの複製が許可され、ある程度のデータの不整合を許容している
  - これは、サービス駆動型アーキテクチャとは対照的
    - サービスを分離して不整合を回避している
    - 疎結合の原則を提唱し、各サービスはデータベースや、レガシーアプリケーション、API などほかの共有リソースへの依存が制限されている

> イベントの例：新規顧客の登録、注文、飛行機の着陸

ソース（イベントプロデューサー）でのイベント発生のたびに、メッセージ（イベント通知）が生成され、ストリーミングプラットフォームに配信される。

ストリーミングプラットフォームにはメディエータートポロジーとブローカートポロジーが存在する。

これらを合わせて Pub-Sub モデル(Publish-Subscribe model)と呼ぶこともある。

Publisher(出版者)と呼ばれるメッセージの送り手が、Subscriber(購読者)の状態を確認せずにメッセージをキューに送信することから、こう呼ばれる

> ※イベント駆動型アーキテクチャによっては、ソース（イベントプロデューサー）と配信先アプリ（イベントコンシューマ）の間に何も挟まずに実装することもできるが、アーキテクチャの管理が非常に難しくなるため推奨されない。

# 5.8 　ガバナンスとセルフサービスモデルのためのメタデータ

ストリーミングアーキテクチャには、メタデータ機能やメタデータ要件が付加できるものもある

特に、先述のスキーマレジストリが重要　スキーマレジストリでは以下を行う

- メッセージキューとトピックの所有権登録
  - メッセージをパブリックとプライベートに振り分けるのに使う
- スキーマドキュメント管理
  - メッセージのスキーマレイアウトをドキュメント化しておく
- スキーマのバージョン管理
  - バージョン管理することで、イベントがどのバージョンのスキーマで処理されたかを記録でき、コンシューマ側での処理に役立つ
  - RDS や API のバージョン管理と同じようなもの
- リネージ
  - イベントを後から追跡するために、一意のリネージ識別番号を組み込む必要がある(uuid のようなもの)
  - バグなどがあったときに、このリネージをもとに追跡する
    - > 例: 追加 → 削除 → 追加　のようなレコードがあったときに、そのままでは 1 つ目と 2 つ目の追加を区別できない。この際、リネージが役に立つ

---

# 6

## 6.1 アーキテクチャの振り返り

- 誰でもデータが利用できるようにするために、下記を守るのがどのアーキテクチャでも重要
  - 内部ドメインとアプリケーションの複雑さは、ほかのドメインから隠さなければならない
  - データレイヤーを介して消費に最適化されたデータを公開する

### 6.1.1 RDS アーキテクチャ

- 大量のデータを扱うユースケース向き(BI や分析など)
- 耐久性、受動性、永続性のあるデータを保持、大量に読み出すことに向く
- 強一貫性が必要ないのであれば API を設けることも可能(RDS アーキテクチャは非同期なので厳密な一貫性はないということ？)

### 6.1.2 API アーキテクチャ

- リアルタイムの同期通信を要するユースケース向き
- データとビジネス機能の両方を提供できる

### 6.1.3 ストリーミングアーキテクチャ

- リアルタイムにデータを変換し、ほかのアプリケーションに通知する
- ストリーミングアーキテクチャで構成したプラットフォーム(データレイヤー)を RDS として使うことも可能
- アプリケーションの状態を伝えるイベントを流通し、(それをトリガーに)データのレプリケーションをとることもできる

#### 本当に重要なのは…

- 各ドメインチームが自分たちに適したアーキテクチャを選択できること
- 各ドメインはアプリケーションをデータレイヤーにのみ公開する

### 6.1.4 強化パターン

- これらのアーキテクチャは組み合わせて使うことでより強力にできる

#### ゲートウェイルーティングとインデックステーブル

- RDS × API
- クエリトラフィック(変更を伴わない)は RDS へ、コマンドトラフィック(変更を伴う)は運用システム(プロバイダー)へルーティングする
- 頻繁に読み出されるデータについて運用システムへのトラフィックを減少させ、クエリ性能も向上する
  - 読み出し専用のレプリケーションを作るイメージ

#### キューベースの負荷平準化

- API × ストリーミング
- API にリクエストが殺到してダウンすることを防ぐために、リクエストをキューで保持する
  - ストリーミングアーキテクチャなんて大仰な言い方しなくても普通のキューの使い方では…？

#### ノーティファイア

- RDS × ストリーミング
- RDS でデータが利用できるようになった際に通知をストリーミングアーキテクチャで行う

#### ストリーミングインジェスト

- RDS × ストリーミング
- ストリーミングアーキテクチャで構成したプラットフォームでアプリケーションの状態を転送して RDS を構築
- 複数のネットワークにまたがる分散型の RDS に使える

#### これらのアーキテクチャは…

- 通常データレイヤーに適用される
- プロバイダーやコンシューマで使われるソフトウェアアーキテクチャパターンも色々ある
- ネットワークの接続性の問題を回避するには非同期通信がおすすめ
- コンテキスト境界を越える(ドメインが異なる)ときにはデータレイヤーを使うこと
  - すべてのメタデータを補足しなければならない
  - データレイヤーを介さない直接通信はインタフェースの変更を検知できないリスクを認識すること

#### 6.2.4.4 例外的な場合

- 唯一データレイヤーを用いるべきでないケースは遅延が問題になるアプリケーション
  - 銀行業界など

### 6.3.1 消費最適化の原則

- データ品質、表現力、粒度、情報のリッチさについては、データ所有者が責任を負わなければならない
- データ所有者の繰り返し作業をなくすために、データは再利用可能で、できるだけ多くの顧客に対応できることが重要
- 複雑なデータを生のまま投入することは、近視眼的には利点があるかもしれないが、データコンシューマに複雑なデータ処理を生じさせるため、避けるべき
- 以下のような設計に従うべき
  - RDS、API、ストリームどのアーキテクチャでも、汎用的なブループリントとしてデータエンドポイントを設計すること
  - 消費最適化すること。再利用しやすく論理的にグループ化された形で提供すること
  - 命名はドメインから継承すること
  - ほかのドメインのニーズに合わせないこと
  - できるだけ多くのドメインが再利用できるようにすること
  - データ利用者が本当に関心のあるデータのみを提供すること(システム内でしか使わないことを提供しない)
  - データ要素はそれ以上意味のある単位に分割できないようにすること
  - 一貫性のあるドメイン ID をつけること
  - データ形式に一貫性をもたせること(表記法や小数点精度など)
  - ローカルでマスタでない参照データは変化しやすいため、安定した粗い粒度にすること
  - マスタデータ管理対象である場合はエンタープライズ ID を提供すること
- どこまでがプロバイダの仕事で、どこまでがコンシューマの仕事かを分離するのは難しく、合意形成が必要
  - 知識、経験、常識に基づくほかない…

### 6.3.2 メタデータの検索性

- データレイヤー用のプラットフォームは誰が提供してもいいが、データエンドポイントの所有権と説明責任はプロバイダーにある
- メタデータをすべてのドメインで利用できるようにする方法のひとつがエンタープライズメタデータモデル
- 密結合を避けるため、メタデータも抽象化しておくこと
  - たとえば所有権情報が物理データ属性に直接関係していると、それが変更された際にすべてのインタフェースも変更しなければならない
- スキーマの進化と互換性のために、インタフェースはバージョン管理されること
- ドメインが自らデータセットやインタフェースを登録できるようなアーキテクチャにしておく

↓ 要するにこういうこと(どないすんねんはあまり書いていなかった)。

> 検索しやすくするために、対応するインタフェースやスキーマ設計など、すべてのメタデータに簡単にアクセスできるようにすることをお勧めします。

### 6.3.3 意味的一貫性

- 一貫性を実現するために、すべてのドメインで、アプリケーションのデータモデルとインタフェースモデルを十分にドキュメント化する必要がある
- 各データ要素が何を意味し、どこで生成され、物理的にどこに保存されているかを理解すること
- 単一のリポジトリでさまざまなモデル間の相互関係を結びつけて、検証するための簡単な方法はない
  - 可能性を高めるために、ツールを開発し、メタデータを組み合わせ、統合する必要がある
  - まずはカタログ、リポジトリ、ドキュメントポータルからはじめるのがいい
- すべてのメタデータを接続するのには、ゴールデンデータセットの要素を参照するためのインタフェーススキーマをプロバイダーから提供してもらうのが賢い
- モデル間の一貫性を確保するための手順として
  - ビジネスエンティティとゴールデンデータセットの要素感の関係がすべて設定されているかどうかを検証する(論理的な対応の確認？)
  - インタフェースモデルや物理データの検証を行う(物理的な対応の確認？)
- データレイヤーへのインタフェースのカバー率の検証も有効

### 6.3.4 対応するメタデータの提供

- スケーラブルなアーキテクチャを実現する大変さの大半は、メタデータ駆動型のアーキテクチャを実現すること
- 検索性を高めるには、最終的にはすべてがカタログ、レジストリ、開発者ポータルに集約されなければならない

### 6.3.5 データの出所と移動

- **リネージ**はデータガバナンスの中でも非常に重要
  - データがどこで生まれ、どのように収集され、どのように下流で消費されたかを把握するための「鉄の弾丸」
  - 使途はコンプライアンス、法規制、倫理、アドバンストアナリティクスモデルの再現性と透明性
    - 要するに機械学習モデルのデータのバージョン管理みたいな話
- 常にリネージを把握するために、必ずデータレイヤーを用いることが原則となる
  - 残念ながら RDS、API、ストリームのすべての統合パターンを網羅してデータリネージを行えるプラットフォームはまだ存在しないため、自分で設計して構築する必要がある(バックエンドくらいは市販品を流用できるかも)
- **リネージの最低限の要件は、どのアプリケーションが、どのプラットフォームを使用して、どのデータを変換したかを特定すること**

## 6.5 まとめ

- Scaled Architecture はどんなタイプのアプリケーションも排除しない
  - マイクロサービスだけを推奨しているわけではない
- ドメインの依存はデータレイヤーとの間のみになり、代わりにメタデータがすべてをまとめる接着剤となる
  - この仕組みをうまく機能させるには、すべてのドメインが消費最適化された再利用可能な方法でデータを提供することが必要

---

7.1 データガバナンス
データガバナンスとデータセキュリティの関係性

許可されたパーティだけがアクセス
認証・認可の統一的な適用
データガバナンスはデータ管理に対する権限と統制を対応する資産含めて実行・強化する活動
⇨ 企業が成長し始めると問題が発生し、必要性が高まる

加えて、法規制に直面し、データがどこに保存され、どこから来たのか、何のために使われているのかドキュメント化することが要求されている。

データガバナンスは５つの側面がある

組織
プロセス
テクノロジー
人
データ
組織の役割
データ所有者：メタデータを管理
データ利用者：要件を設定
データ作成者：データを作成する内部・外部パーティ
データコンシューマー：データ利用者が意図した通りにデータを利用する内部・外部パーティ
アプリケーション所有者：アクセス制御の管理
データスチュワード：データポリシーとデータ標準の遵守
ガイドライン、ルール、活動、役割、責任を設定

大企業では、非中央集権的に各ドメインの人々が行う連合型組織になっている

データガバナンス部門が様々な役割を割り当てる場合、その責任とタスクが説明されていることが重要

図 7.1

データを明確に定義し、説明責任を果たし、その目的について合意が必要
ビジネス上のステークホルダーが関与する
技術視点だとインタフェース、SLA、プロトコル、バージョン管理

---

# 8 章 データを価値に変える

- データを価値に変えるために考えること
  - ビジネス要件
    - 要件に応じて、ビジネスインテリジェンス、リアルタイム意思決定、機械学習など、様々な手法を使える
  - 非機能的要件
    - データ変換の種類、速度、並列化のための最適化、消費パターン

## 8.4 ビジネス要件

- ビジネス上の目的と目標が明確に定義され、詳細に記述され、完全に定義されていることを確認することが、ソリューションの基礎になる
  - どのようなビジネス上の問題を解決する必要がるか
  - どのようなデータソースが必要なのか
  - どのようなソリューションが利用可能なのか
  - リアルタイムやオフラインで実行しなければならないデータ処理とは何か
  - 一貫性と要件は何か
  - 他のドメインが再利用することが可能な成果物とは何か
    - ?他のドメインが再利用できる必要は有るのか?

## 8.5 非機能要件

- ソリューション構築のビジネス要件定義の次は、非機能要件を明確にすること
- 非機能要件の例
  - コスト
  - スケーラビリティ
  - 性能
  - 遅延
  - 保守性
  - 容量
  - 多様性
  - 速度
  - 一貫性
  - セキュリティとガバナンス
  - 書き込みと呼び出しの特性

---

# 9 エンタープライズデータ資産の活用

- 企業内の様々なドメイン間でデータのコンテキストに一貫性がなく，データ品質のレベルにばらつきがあることが問題になることがある
- この課題に対処するにはマスターデータ管理(MDM)が必要
  - MDM は重要なデータを管理／流通させ，一貫性，品質，信頼性を確保するもの
  - データ保護法などにも効果がある
    - マスターデータの管理が不十分だと，不正の検出ができなかったり，規制当局から罰則を受けたりする可能性がある

## 9.1 マスターデータ管理について

- ほとんどの企業は顧客や製品などの共通点のあるデータリストを共有するシステムを持っている
  - これらのデータは重複している可能性があり，非効率だったり矛盾が生じたりする
- MDM はアプリケーションやシステム全体に対して，マスター管理しているバージョンのデータを流通させることで，データの不整合を検出し，解決する

---

## 9.3 MDM 参照アーキテクチャ

- MDM のプロセスはどのようなアプリケーションで,どのような一意の信頼できるデータが作成され，管理されているのかを特定するところから始める
- データはアプリケーションごとに異なる形式で保存されるため，それぞれのコンテキストを理解し，どのようなビジネスルール，形式，参照範囲がそのマスターデータに影響を与えるのかを把握しておくことが重要
- 重複する部分を識別したら，今度はデータレイヤーに向けて情報提供を開始する
- このプロセスではメタデータも配信される必要がある

### 9.3.3 マスター識別番号

- MDM の重要な要素として，マスターデータとローカルシステムからのデータを関連付けるマスター識別番号がある
- このマスター識別番号はどのデータがマスター管理されているのかを把握するために必要
- 一意のデータを識別し，マスター識別子を割り当てることはシステム内でローカルに行うのではなく，グローバルにのみ行うことができる
- そのためには様々なシステムからの全てのデータを再送信して再び認識してもらわないと行けない
- マスター識別子を配信する場合，MDM のマスター識別子を全ての管理部門に通知し，不整合の問題がおこならないようにすると良い
- マスターデータ管理の対象となる管理部門のみが MDM ハブからマスター識別子を取得する必要がある

  - MDM の対象でないシステムは独自のローカルな完全性を利用すること

- 所感
  - 大規模な会社にはなかな難しそう
  - トップダウンでやるべしなのかな

### 9.3.4 参照データとマスターデータ

- 参照データとマスターデータは明確に区別することがおすすめ
- 参照データとは，他のデータを定義，分類，整理，グループ化，カテゴリするために使用されるデータのこと
- マスターデータはコアとなる概念に関するもの
- 参照データの原則は，データレイヤーでデータを流通する際にはマスター識別子を使用すること
- メタデータとデータ品質コントロールを使えば，同一の一貫性のあるデータを使用できるのでデータ品質を保つことができる

## 9.4 エンタープライズデータのスコープの決定

- マスターデータと参照データの組織的なスコープをどのように定義するかということが難しい
- MDM ではエンタープライズデータを統一するという落とし穴にはまりがち
- スコープを広げ，結果的にあまりにも多くのデータをマスター管理してしまうと，データ統合，ガバナンス，調整作業が急激に膨れ上がってしまうから
- メタデータを使って，リネージ，データモデル，共有合意からドメイン間の重複部分や共通の関心事を見つけることができる

  - そしてスコープを決定することができる
  - メタデータの品質が高くなったら，機械学習を使ってどこに重なる部分があるか，どのデータを企業レベルで管理すべきかを推定することを検討すると良い
    - 筆者曰くできるのではないかとの意見

- 様々なレベルの重複が発見される場合がある
  - データの重複程度と，重要度を区別する

### エンタープライズデータ

- 企業全体に及ぶスコープと用途のあるデータのこと
- マスターデータと参照データの両方が含まれる
- その一貫性は重要であり，そのスコープはいくつかのドメインから全てのドメイン，あるいは企業の外まで様々に変化する場合がある

### ドメインデータ

- 一部のドメイン間で共有されるデータであり，全てのドメインで共有されるわけでないデータ
- データが重複するドメイン群にとっては管理することが重要だが，企業全体としての重要性はなく，法規制上の重要性もないデータ
- ドメインデータはいくつかのドメイン内で管理／保守されるが，グローバルレベルでは管理されない

### ローカルデータ

- 他のドメインで共有されたり，使用されたりしないデータ
- 一つのローカルドメインやシステムの中でのみ使用される

## 9.5 サービスとしての MDM とデータ品質

- 企業やドメインでの MDM の導入を成功させるためには，MDM 製品をドメインにサービスとして提供することを考えること
- MDM ソリューションは複雑で，導入が難しい
- インフラストラクチャーを抽象化して，MDM をサービスとしてドメインに提供することで非常に簡単に使える場合がある

## 9.6 キュレーションデータ

- マスターデータ管理はデータキュレーションと似ているところがある
- どちらも多様なソースからデータを収集し，それを統合することで，個々のデータよりも高い価値を持つようにするプロセス
- データキュレーションは他のチームが繰り返し行う統合作業を排除するもの
- MDM もキュレーションデータ作成も価値を提供し，データコンシューマの作業を楽にすることを目的としている
  - ETL,データクレンジング，データサイエンス，メタデータなどの技術を共通化する
- データキュレーションが MDM と異なるのは，既存のデータを修正するのか，あらたにデータを作成するのかが明示されていない点
  - データキュレーションであればコンテキストの変更が可能
- データキュレーションの大部分はスキーマ，テーブル，カラム情報，クエリ，などのメタデータの整理から始まる

### 9.6.1 メタデータ交換

- データに企業レベルでの意味的一貫性を持たせる方法として，メタデータを共有する方法がある
  - この方法ではデータそのものは変更や共有はされない
- メタデータを様々なドメインと共有するには，いくつかの方法がある
  - データカタログ
  - メタデータにはある特定のデータエンティティに関する情報(場所，アノテーション，属性，関係，意味など)を含めておく
  - エンティティが類似している場合はラベルやアノテーションを使って同じように表現できる

## 9.7 データガバナンスとの関係

- データキュレーションとマスターデータ管理との違いは，データガバナンスにとって重要になる
- MDM では，一般的にデータは一貫性の観点から調整される
- MDM は意味を変えて新しいデータを生成するわけでないので，データ所有者と管理部門は常に元のゴールデンソースシステムのデータ所有者まで辿ることができる
- 一方データキュレーションは既存のデータから新しいデータを作成または導入する場合があるため，新しいデータに対して所有権が発生することがある
- そのためデータキュレーションはドメインとより強く連携し，ドメイン内で実行される

## 9.8 まとめ

- マスターデータ管理が重要であることは明らか

  - なぜならユーザーは使用するデータに一貫性と正確性があればこそ，正しい判断を下すことができるから
  - MDM はエンタープライズレベルでの一貫性と品質を保証している

- MDM の導入を始めるにあたって，最もシンプルな導入スタイルである，リポジトリから始めるのが現実的な方法
- このスタイルでは運用システムと調整することなく，どのデータを調合する必要があるのかや，どのデータの品質が悪いかを知ることによって，迅速に価値を提供することができる
- 次のステップはスコープをはっきりさせること

  - 全てのデータを選択しようとするとだめ．企業レベルのデータ統一は落とし穴
  - 組織にとって価値の高いところから始めるべし

- 最後のステップは最終的な目標である共存を実現すること
  - つまり，改善されたものが元のソースシステムに直接戻ってくるようにすること

---

# 10 章 メタデータによるデータの民主化

- メタデータは、本書でここまで紹介してきたアーキテクチャに必要な情報をすべて提供する
- メタデータは各ツールに散在しているので、整理して統合しないといけない

---

筆者の経験上、メタデータ機能に対して互換性のないテクノロジーが既に社内に乱立していると、コントロールされた環境を作ろうという意気込みが削がれる

大きく 3 つの目標に絞って考えるのがよい

- 10-2. すべての企業領域とその属性及び関係を表す、エンタープライズメタデータモデルを構築する
- 10-3. 重要なメタデータのコレクションを定義し、API やラッパー、ストリームによってメタデータを収集・公開するために最適なアーキテクチャを決定します
- 10-4. セルフサービス機能とポータルを用いて、メタデータを民主化する

# 10.2 　エンタープライズメタデータモデル

最初はシンプルで小規模なものから始める

## 始めるうえで検討すべき観点

- どのようなビジネスメタデータが重要か
- システムの相互運用に必要なメタデータは何か
- どのようなプロセスやストリームでデータを取り込んでいるか
- モデルやスキーマはどこで作成され、管理されているか
- データガバナンス部門が適切に業務を遂行するために、どのような情報を、チームが中央に提供する必要があるか

上記の検討の後、各メタデータストリームについてコンテンツのライフサイクルをかき、すべての依存関係を明らかにする

→ 特定ベンダーに依存しない統一メタデータモデルが出来上がる

## まずまとめるべきところ

- アプリケーションのリスト
- 権威あるデータソース（ゴールデンソース）
- データベースとインタフェースのスキーマ
- データの所有権
- セキュリティ

## メタデータ管理の主要な対象領域をすべて示した概略図

主要なものだけでも、これだけ多くの領域がある

<img src="fig/10-2.png" width="480px">
