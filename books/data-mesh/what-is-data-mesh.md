One long-standing challenge of existing analytical data architectures is the high friction and cost of using data: discovering, understanding, trusting, exploring, and ultimately consuming quality data. There have been numerous surveys surfacing this friction. A recent report from Anaconda, a data science platform company, “The State of Data Science 2020”, finds that nearly half of a data scientist’s time is spent on data preparation—data loading and cleansing. If not addressed, this problem only exacerbates with data mesh, as the number of places and teams who provide data, i.e., domains, increases. Distribution of the organization’s data ownership into the hands of the business domains raises important concerns around accessibility, usability, and harmonization. Further data siloing and regression of data usability are potential undesirable consequences of data mesh’s first principle, domain-oriented ownership. The principle of data as a product addresses these concerns. The second principle of data mesh, data as a product, applies product thinking to domain-oriented data to remove such usability frictions and truly delight the experience of the data users—data scientists, data analysts, data explorers, and anyone in between. Data as a product expects that the analytical data provided by the domains is treated as a product, and the consumers of that data should be treated as customers—happy and pleased. Furthermore, data as a product underpins the case for data mesh, unlocking the value of an organization’s data by dramatically increasing the potential for serendipitous and intentional use. In his book INSPIRED, Marty Cagan, a prominent thought leader in product development and management, provides convincing evidence on how successful products have three common characteristics: they are feasible, valuable, and usable. Data as a product principle defines a new concept, called data product, that embodies standardized characteristics to make data valuable and usable. Figure 3-1 demonstrates this point visually. This chapter introduces these characteristics. Chapter 4 describes how to make building data products feasible. Figure 3-1. Data products live at the intersection of Marty Cagan’s characteristics of successful products For data to be a product it adheres to a set of rules and exhibits a set of traits that make it fit right in the intersection of Cargan’s usability, feasibility, and valuable Venn diagram. For data to be a product, it must be valuable to its users—on its own and in cooperation with other data products. It must demonstrate empathy for its users and be accountable for its usability and integrity. I admit that treating data as a product doesn’t simply happen out of good intentions. For that, this principle introduces new roles to domains such as the domain data product owner and data product developer who have responsibility for creating, serving, and evangelizing data products, while maintaining the specific objective measures of data accessibility, quality, usability, and availability over the lifetime of the data products. Chapter 16 will cover further details about these new roles. Compared to past paradigms, data as a product inverts the model of responsibility. In data lake or data warehousing architectures the accountability of creating data with quality and integrity resides downstream from the source and remains with the centralized data team. Data mesh shifts this responsibility close to the source of the data. This transition is not unique to data mesh; in fact, over the last decade we have seen the trend of shift left with testing and operations, on the basis that addressing problems is cheaper and more effective when done close to the source. I go as far as saying that what gets shared on a mesh is not merely data; it is a data product. Note Data as a product is about applying product thinking to how data is modeled and shared. This is not to be confused with product selling. Let’s unpack how we can apply product thinking to data. Applying Product Thinking to Data Over the last decade, high-performing organizations have embraced the idea of treating their internal operational technology like a product, similarly to their external technology. They treat their internal developers as customers and their satisfaction a sign of success. Particularly in two areas, this trend has been strongly adopted: applying product management techniques to internal platforms, which accelerates the ability of internal developers to build and host solutions on top of internal platforms (e.g., Spotify Backstage), and treating APIs as a product to build APIs that are discoverable, understandable, and easily testable to assure an optimal developer experience (e.g., Square APIs). Applying the magic ingredient of product thinking to internal technology begins with establishing empathy with internal consumers (i.e., fellow developers), collaborating with them on designing the experience, gathering usage metrics, and continuously improving the internal technical solutions over time to maintain ease of use. Strong digital organizations allocate substantial resources and attention to building internal tools that are valuable to the developers and ultimately the business. Curiously, the magical ingredient of empathy, treating data as a product and its users as customers, has been missing in big data solutions. Operational teams still perceive their data as a byproduct of running the business, leaving it to someone else, e.g., the data team, to pick it up and recycle it into products. In contrast, data mesh domain teams apply product thinking with similar rigor to their data, striving for the best data user experience. Consider Daff. One of its critical domains is the media player. The media player domain provides essential data such as what songs have been played by whom, when, and where. There are a few different key data users for this information. For example, the media player support team is interested in near-real-time events to quickly catch errors causing a degrading customer experience and recover the service, or respond to incoming customer support calls informedly. On the other hand, the media player design team is interested in aggregated play events that tell a data story about the listener’s journey over a longer period of time, to improve media players with more engaging features toward an overall better listener experience. With an informed understanding of these use cases and what information the other teams need, the media player domain provides two different types of data as its products to the rest of the organization: near-real-time play events exposed as infinite event logs, and aggregated play sessions exposed as serialized files on an object store. This is product ownership applied to data. Figure 3-2 shows the media player domain data products. Figure 3-2. Data products example As you can imagine, you can adopt the majority of the product ownership techniques to data. However, there is something unique about data. The difference between data product ownership and other types of products lies in the unbounded nature of data use cases, the ways in which particular data can be combined with other data and ultimately turned into insights and actions. At any point in time, data product owners are aware or can plan for what is known today as viable use cases of their data, while there remains a large portion of unknown future use cases for the data produced today, perhaps beyond their imagination. This is more true for source-aligned domains and less for consumer-aligned ones. This source-aligned data captures the reality of the business interactions and events as they have happened. They can continue to be used, transformed, and reinterpreted by data users of the future. Source-aligned data products need to balance the immediate known use cases and the unknown ones. They have no choice but to strive to model the reality of the business, as closely as possible, in their data, without too much assumption in how it will be used. For example, capturing all the play events as an infinite high-resolution log is a safe choice. It opens the spectrum of future users who can build other transformations and infer new insights from the data that is captured today. This is the main difference between data product design and software product design. Baseline Usability Attributes of a Data Product In my opinion, there is a set of non-negotiable baseline characteristics that a data product incorporates to be considered useful. These characteristics apply to all data products, regardless of their domain or archetype. I call these baseline data product usability attributes. Every data product incorporates these characteristics to be part of the mesh. Figure 3-3 lists data products’ usability attributes. Figure 3-3. The baseline usability attributes of data products (DAUTNIVS) Note that these usability characteristics are oriented around the experience of the data users. These are not intended to express the technical capabilities. Part IV, “Design of Data Quantum Architecture,” covers the technical characteristics of data products. The baseline characteristics listed in this section are an addition to what has been known as FAIR data in the past—data that meets the principles of findability, accessibility, interoperability, and reusability.1 In addition to these principles I have introduced characteristics that are necessary to make distributed data ownership work. Let’s put ourselves in the shoes of data users and walk through these attributes. Discoverable Two of the most important characteristics of good design are discoverability and understanding. Discoverability: Is it possible to even figure out what actions are possible and where and how to perform them? Understanding: What does it all mean? How is it supposed to be used? What do all the different controls and settings mean?2 ​—​Don Norman The very first step that data users take in their journey is to discover the world of data available to them and explore and search to find “the one.” Hence, one of the first usability attributes of data is to be easily discoverable. Data users need to be able to explore the available data products, search and find the desirable sets, and explore them and gain confidence in using them. A traditional implementation of discoverability is a centralized registry or catalog listing available datasets with some additional information about each dataset, the owners, the location, sample data, etc. Often this information is curated after the fact by the centralized data team or the governance team. Data product discoverability on data mesh embraces a shift-left solution where the data product itself intentionally provides discoverability information. Data mesh embraces the dynamic topology of the mesh, continuously evolving data products and the sheer scale of available data products. Hence, it relies on individual data products to provide their discoverability information at different points of their life cycle—build, deploy, and run—in a standard way. Each data product continuously shares its source of origin, owners, runtime information such as timeliness, quality metrics, sample datasets, and most importantly information contributed by their consumers such as the top use cases and applications enabled by their data. Part IV, “How to Design the Data Product Architecture”, will discuss the technical design of data product discoverability. Addressable A data product offers a permanent and unique address to the data user to programmatically or manually access it. This addressing system embraces the dynamic nature of the data and the mesh topology. It must recognize that many aspects of data products will continue to change, while it assures continuity of usage. The addressing system accommodates the following ever-changing aspects of a data product, among others, while retaining access to the data product through its long-lasting unique address: Semantic and syntax changes in data products Schema evolution Continuous release of new data over time (window) Partitioning strategy and grouping of data tuples associated with a particular time (or time window) Newly supported modes of access to data New ways of serializing, presenting, and querying the data Changing runtime behavioral information For example, service-level objectives, access logs, debug logs The unique address must follow a global convention that helps users to programmatically and consistently access all data products. The data product must have an addressable aggregate root that serves as an entry to all information about a data product, including its documentation, service-level objectives, and the data it serves. Understandable Once a data product is discovered, the next step of the data user’s journey is to understand it, which involves getting to know the semantics of its underlying data, as well as various syntax in which the data is encoded. Each data product provides semantically coherent data: data with a specific meaning. A data user needs to understand this meaning: what kind of entities the data product encapsulates, what the relationships among the entities are, and their adjacent data products. Getting back to the media player event example, a data user should easily understand what constitutes a player event: the user, the play actions they have taken, the time and location of their action, and the feedback the action has resulted in. The data user should easily understand the kinds of actions available and that there is a relationship between a listener triggering a player event and a subscriber from the adjacent listener domain. Data products provide a formal representation of such semantics. In addition to understanding the semantics, data users need to understand how exactly the data is presented to them. How is it serialized, and how can they access and query the data syntactically? What kind of queries can they execute or how can they read the data objects? They need to understand the schema of the underlying syntax of data. Sample datasets and example consumer codes ideally accompany this information. Examples accompanied with a formalized description of the data improve data users’ understanding. Additionally, dynamic and computational documents like computational notebooks are great companions to tell the story of a data product. Computational notebooks include documentation of the data, as well as code to use it, with immediate feedback that visually demonstrates the code’s result. Lastly, understanding is a social process. We learn from each other. Data products facilitate communication across their users to share their experience and how they take advantage of the data product. Understanding a usable data product requires no hand-holding. A self-serve method of understanding is a baseline usability characteristic. Trustworthy and truthful [Trust is] a confident relationship with the unknown.3 ​—​Rachel Botsman No one will use a product that they can’t trust. So what does it mean to trust a data product, and more importantly what does it take to trust? To unpack this, I like to use the concept of trust offered by Rachel Botsman: the bridge between the known and the unknown. A data product needs to close the gap between what data users know confidently about the data, and what they don’t know but need to know to trust it. While the prior characteristics like understandability and discoverability close this gap to a degree, it takes a lot more to trust the data for use. The data users need to confidently know that the data product is truthful, that it represents the fact of the business correctly. They need to confidently know how closely data reflects the reality of the events that have occurred, the probability of truthfulness of the aggregations and projections that have been created from business facts. A piece of closing the trust gap is to guarantee and communicate data products’ service-level objectives (SLOs)—objective measures that remove uncertainty surrounding the data. Data product SLOs include, among others: Interval of change How often changes in the data are reflected Timeliness The skew between the time that a business fact occurs and becomes available to the data users Completeness Degree of availability of all the necessary information Statistical shape of data Its distribution, range, volume, etc. Lineage The data transformation journey from source to here Precision and accuracy over time Degree of business truthfulness as time passes Operational qualities Freshness, general availability, performance In the traditional approaches to data management, it’s common to extract and onboard data that has errors, does not reflect the truth of the business, and simply can’t be trusted. This is where the majority of the efforts of centralized data pipelines are concentrated, cleansing data after ingestion. In contrast, data mesh introduces a fundamental shift that the owners of the data products must communicate and guarantee an acceptable level of quality and trustworthiness—specific to their domain—as an intrinsic characteristic of their data product. This means cleansing and running automated data integrity tests at the point of the creation of a data product. Providing data provenance and data lineage—the data journey, where it has come from and how it got here—as the metadata associated with each data product helps consumers gain further confidence in the data product. The users can evaluate this information to determine the data’s suitability for their particular needs. I’m of the opinion that once the discipline of building trustworthiness in each data product is established, there is less need for establishing trust through investigative processes and applying detective techniques navigating a lineage tree. Having said that, data lineage will remain an important element in a few scenarios, such as postmortem root-cause analysis, debugging, data compliance audits, and evaluation of data’s fitness for ML training. Natively accessible Depending on the data maturity of the organization there is a wide spectrum of data user personas in need of access to data products. The spectrum spans a large profile of users, as displayed in Figure 3-4: data analysts who are comfortable with exploring data in spreadsheets, data analysts who are comfortable with query languages to create statistical models of data as visualizations or reports, data scientists who curate and structure data and consume data frames to train their ML models, and data-intensive application developers who expect a real-time stream of events or pull-based APIs. This is a fairly wide spectrum of users with equally diverse expectations of how to access and read data. Figure 3-4. Example of a wide spectrum of data product users with different data access patterns There is a direct link between the usability of a data product and how easily a particular data user can access it with their native tools. Hence, a data product needs to make it possible for various data users to access and read its data in their native mode of access. This can be implemented as a polyglot storage of data or by building multiple read adapters on the same data. For example, the play events data product needs to natively support reading data through SQL query to satisfy a data analyst’s native mode of access, publishing a stream of events for data-intensive applications, and columnar files for data scientists. Interoperable One of the main concerns in a distributed data architecture is the ability to correlate data across domains and stitch them together in wonderful and insightful ways: join, filter, aggregate. The key for an effective composability of data across domains is following standards and harmonization rules that allow linking data across domains easily. Here are a few things data products need to standardize to facilitate interoperability and composability: Field type A common explicitly defined type system Polysemes identifiers Universally identifying entities that cross boundaries of data products Data product global addresses A unique global address allocated to each data product, ideally with a uniform scheme for ease of establishing connections to different data products Common metadata fields Such as representation of time when data occurs and when data is recorded Schema linking Ability to link and reuse schemas—types—defined by other data products Data linking Ability to link or map to data in other data products Schema stability Approach to evolving schemas that respects backward compatibility For example, let’s look at managing polysemes identifiers. At Daff, artist is a core business concept that appears in different domains. An artist, while remaining the same global entity, has different attributes and likely different identifiers in each domain. The play events data product recognizes the artist differently from the payment domain—taking care of invoices and payments for artists’ royalties. In order to correlate data about an artist across different domain data products Daff needs to agree on how they identify an artist, globally, across all data products. Chapter 5 covers the topic of global standards and protocols applied to each data product. Interoperability is the foundation of any distributed system design, and data mesh is no exception. Valuable on its own I think it’s fairly obvious that a data product must be valuable—it should have some inherent value for the data users in service of the business and customers. After all, if the data product owner can’t envisage any value out of the data product, perhaps it’s best not to create one. Having said that, it’s worth calling out that a data product should carry a dataset that is valuable and meaningful on its own—without being joined and correlated with other data products. Of course, there is always higher-order meaning, insight, and value that can be derived by correlating multiple data products. However, if a data product on its own serves no value on its own, it should not exist. While this may sound obvious, there is a common antipattern when migrating from a warehouse architecture to data mesh: directly mapping warehouse tables to data products can create data products with no value. In the data warehouse, there are glue (aka facts) tables that optimize correlation between entities. These are identity tables that map identifiers of one kind of entity to another. Such identity tables are not meaningful or valuable on their own—without being joined to other tables. They are simply mechanical implementations to facilitate joins. On the contrary, there are no mechanical data products that solely exist to enable the machines to correlate information across the mesh. Machine optimizations such as indices or fact tables must be automatically created by the platform and hidden from the product products. Secure Data users access a data product securely and in a confidentiality-respecting manner. Data security is a must, whether the architecture is centralized or distributed. However, in a distributed architecture like data mesh, the access control is validated by the data product, right in the flow of data, access, read, or write. The access control policies can change dynamically. They continuously get evaluated at every point of access to data products. Additionally, access to a data product is not quite binary—whether the user can see or can’t see the data. In many cases while the user may not be able to see the actual records, it might have sufficient permissions to see the shape of the data and evaluate it using its statistical characteristics. Access control policies can be defined centrally but enforced at runtime by each individual data product. Data products follow the practice of security policy as code. This means to write security policies in a way that they can be versioned, automatically tested, deployed and observed, and computationally evaluated and enforced. A policy described, tested, and maintained as code can articulate various security-related concerns such as the following, among others: Access control Who, what, and how data users—systems and people—can access the data product Encryption What kinds of encryption—on disk, in memory, or in transit—using which encryption algorithm, and how to manage keys and minimize the radius of impact in case of breaches Confidentiality levels What kinds of confidential information, e.g., personally identifiable information, personal health information, etc., the data product carries Data retention How long the information must be kept Regulations and agreements GDPR, CCPA, domain-specific regulations, contractual agreements Transition to Data as a Product In working with my clients, I have found that they are overwhelmingly receptive to data mesh principles, often questioning “Why didn’t I think of it myself?” or occasionally saying, “We have been doing something similar but not quite the same.” The principles appear to be intuitive and rather natural next steps in their organization’s tech modernization journey, an extension to modernization of the operational aspect of organizations, e.g., moving toward domain-oriented ownership of capabilities with microservices and treating internal products like operational APIs as products. However, a sense of discomfort arises when they go deeper into what it actually takes to implement the transformation toward data mesh. I found in my conversations with data mesh early implementers that while they verbalize the principles and their intention to implement them, the implementations remain heavily influenced by the familiar techniques of the past. For this reason, I have decided to include a number of thought-provoking transition statements as well as a few pragmatic steps to crystalize the differences between the existing paradigm and truly owning data as a product. I invite you to think of new transition statements that I likely have missed here. Include Data Product Ownership in Domains In the past decade, teams have continued to move from being functionally divided to becoming cross-functional. The DevOps movement has led to closing the gap between building and operating business services and forming dev and ops cross-functional teams. Customer-obsessed product development has brought the design and product ownership closer to the developers. The introduction of analytical data as a product adds to the list of existing responsibilities of cross-functional domain teams and expands their roles to: Data product developer The role responsible for developing, serving, and maintaining the domain’s data products as long as the data products live and are being used Data product owner The role accountable for the success of a domain’s data products in delivering value, satisfying and growing the data users, and maintaining the life cycle of the data products Define these roles for each domain and allocate one or multiple people to the roles depending on the complexity of the domain and the number of its data products. Reframe the Nomenclature to Create Change One commonly used term in data analytics is ingestion, receiving data from an upstream source—often an untrustworthy source that has egested data as a byproduct of its operation. It’s now the job of the downstream pipeline to ingest, clean, and process the data before it can be consumed to generate value. Data mesh suggests reframing receiving upstream data from ingestion to consumption. The subtle difference is that the upstream data is already cleansed, processed, and served ready for consumption. The change of language creates a new cognitive framing that is more aligned with the principle of serving data as a product. Relatedly, the word extraction used in ETL (extract, transform, load) and its other variations needs to be critically evaluated. Extraction evokes a passive role for the provider and an intrusive role for the consumer. As we know, extracting data from an operational database that is not optimized for external use other than its own application creates all kinds of pathological coupling and a fragile design. Instead, we can shift the language to publish, serve, or share. This implies shifting the implementation of data sharing from accessing raw databases to intentionally sharing domain events or aggregates. By now you probably have picked up on my emphasis on language and the metaphors we use. George Lakoff—professor of cognitive science and linguistics at UC Berkeley—in his book, Metaphors We Live By, elegantly demonstrates the consequence of shifting our language around the concept of argument, from war to dance. Imagine the world we would live in and the relationships we would nurture, if instead of winning an argument, losing and gaining argument ground, and attacking the weak points of an argument, we would, as dancers, perform a balanced and aesthetically pleasing argument, expressing our ideas and emotions through the beautiful and collaborative ritual of dancing. This unexpected reframing of language has a profound behavioral impact. Think of Data as a Product, Not a Mere Asset “Data is an asset.” “Data must be managed like an asset.” These are the phrases that have dominated our vernacular in big data management. The metaphor of asset used for data is nothing new. After all, for decades, TOGAF, a standard of the Open Group for Enterprise Architecture methodologies and frameworks, explicitly has penciled in “Data Is an Asset” as the first principle of its data principles. While this is a rather harmless metaphor on the surface, it has shaped our perceptions and actions toward negative consequences, for example, our actions toward how we measure success. Based on my observations, data as an asset has led to measuring success by vanity metrics—metrics that make us look or feel good but don’t impact performance—such as the number of datasets and tables captured in the lake or warehouse, or the volume of data. These are the metrics I repeatedly come across in organizations. Data as an asset promotes keeping and storing data rather than sharing it. Interestingly, TOGAF’s “Data Is an Asset” principle is immediately followed by “Data Is Shared.” I suggest the change of metaphor to data as a product, and a change of perspective that comes with that, for example, measuring success through adoption of data, its number of users, and their satisfaction using the data. This underscores sharing the data versus keeping and locking it up. It puts emphasis on the continuous care that a quality product deserves. I invite you to spot other metaphors and vocabulary that we need to change to construct a new system of concepts for data mesh. Establish a Trust-But-Verify Data Culture Data as a product principle implements a number of practices that lead to a culture where data users, by default, can trust the validity of the data and put their focus on verifying its fitness for their use cases. These practices include introducing a role for long-term ownership of a data product, accountable for the integrity, quality, availability, and other usability characteristics of the data product; introducing the concept of a data product that not only shares data but also explicitly shares a set of objective measures such as timeliness, retention, and accuracy; and creating a data product development process that automates testing of the data product. Today, in the absence of these data-as-a-product practices, data lineage remains a vital ingredient for establishing trust. Data users have been left with no choice but to assume data is untrustworthy and requires a detective investigation through its lineage before it can be trusted. This lack of trust is the result of the wide gap between data providers and data users due to the data providers’ lack of visibility to the users and their needs, lack of long-term accountability for data, and the absence of computational guarantees. Data-as-a-product practices aim to build a new culture, from presumption of guilt to the Russian proverb of trust but verify. Join Data and Compute as One Logical Unit Let’s do a test. When you hear the word data product, what comes to your mind? What shape? What does it contain? How does it feel? I can guarantee that a large portion of readers imagine static files or tables, columns and rows, some form of storage medium. It feels static. It’s accumulated. Its content is made of bits and bytes that are representative of the facts, perhaps beautifully modeled. That is intuitive. After all, by definition datum—singular form—is a “piece of information.”4 This perspective results in the separation of code (compute) from data, in this case, separation of the code that maintains the data, creates it, and serves it. This separation creates orphaned datasets that decay over time. At scale, we experience this separation as data swamps—a deteriorated data lake. Data mesh shifts from this dual mode of data versus code to data and code as one architectural unit, a single deployable unit that is structurally complete to do its job, providing the high-quality data of a particular domain. One doesn’t exist without the other. Data and code coexisting as one unit is not a new concept for people who have managed microservices architecture. The evolution of operational systems has moved to a model in which each service manages its code and data, schema definition, and upgrades. The difference between an operational system is the relationship between the code and its data. In the case of microservices architecture, data serves the code. It maintains state so that code can complete its job, serving business capabilities. In the case of a data product and data mesh this relationship is inverse: code serves data. The code transforms the data, maintains its integrity, governs its policies, and ultimately serves it. Note that the underlying physical infrastructures that host code and data are kept separate. Recap The principle of data as a product is a response to the data siloing challenge that may arise from the distribution of data ownership to domains. It is also a shift in the data culture toward data accountability and data trust at the point of origin. The ultimate goal is to make data simply usable. The chapter explained eight nonnegotiable baseline usability characteristics of data products including discoverability, addressability, understandability, trustworthiness, native accessibility, interoperability, independently valuable, and security. I introduced the role of data product owner—someone with an intimate understanding of the domain’s data and its consumers—to assure continuity of ownership of data and accountability of success metrics such as data quality, decreased lead time of data consumption, and in general data user satisfaction through net promoter score. Each domain includes a data product developer role, responsible for building, maintaining, and serving the domain’s data products. Data product developers will be working alongside their fellow application developers in the domain. Each domain team may serve one or multiple data products. It’s also possible to form new teams to serve data products that don’t naturally fit into an existing operational domain. Data as a product creates a new system of the world, where data is and can be trusted, built, and served with deep empathy for its users, and its success is measured through the value delivered to the users and not its size. This ambitious shift must be treated as an organizational transformation. I will cover the organizational transformation in Part V of this book. It also requires an underlying supporting platform. The next chapter looks into the platform shift to make data as a product feasible.
