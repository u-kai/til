# Automated Placement

- なんか 1.23 以降から新いスケージュールの仕組みがでたのか？
- PodTopologySpread plugin とかがあるらしい
  - ではなく、上書きできる機能ができたのか？
  - 多分オーバーライドする仕組みだわ
  - 例として PodTopologySpread がオーバーライドされている感じだわ
- っていうか pod のスケジューリング機能って

```
apiVersion: kubescheduler.config.k8s.io/v1
kind: kubeSchedulerConfiguration
```

って言うリソースで提供されていることは知らんかった

- 名前が一位であればいくつかのスケージュール方法を設定できるっぽい
- pod は.spec.schedulerName で指定したいスケジュール名を書くみたい
- Scheduling,placement,node assignment,binding

- nodeSelector で disktype とか選べるのはいいよな

  - カスタムラベルも使えるよん

- node affinity でより柔軟な Pod の Node 割り当てができる
- 何かのデータが以上、いかだった Node とか、ラベルの値が含まれる、含まれないとか
- Pod Affinity 葉もっと協力

  - node selector で満足できないときに使う？

- node の特性に関わらないものを条件に入れることができる感じかもしれない
- うまく Pod を分散させて障害があったときにも Pod が全滅しない様にするのは大事
- topologySpreadConstraints を使うと上記ができるのか？
- maxSkew の数が最大の分散数らしいのでこれを 1 にしたらすべての node で 1 以上は配置できないってこと?
  - node 数以上を deployment で配置したらどうなるのかな？
  - おそらく deploy できない気がしている

配置は、Pod をノードに割り当てる芸術です。可能な限り最小限の介入を持つことが望ましいです。複合した複数の設定が予測しきれない可能性があるからです。単純な場面では、リソース制約に基づいて Pod をスケジューリングするだけで十分なはずです。第 2 章「予測可能な要求」のガイドラインに従い、コンテナの全てのリソースニーズを宣言すれば、スケジューラーは仕事をして Pod を可能な最適なノードに配置します。しかし、より現実的なシナリオでは、データのローカリティや Pod のコロケーション、アプリケーションの高可用性、効率的なクラスタリソース利用などの制約に従って、特定のノードへの Pod のスケジュールが必要な場合があります。これらの場合、希望するデプロイメントトポロジにスケジューラを向けるための複数の方法があります。図 6-3 は、Kubernetes の異なるスケジューリング技術を理解し、考えるための一つのアプローチを示しています。

まず、Pod とノードの間の力と依存性を特定します（例えば、専用ハードウェア能力や効率的なリソース利用に基づいて）。次に、次のノード親和性技術を使って Pod を希望するノードに向けるか、または対親和性技術を使って Pod を望ましくないノードから遠ざけます：
nodeName このフィールドは、Pod をノードにハードワイヤリングするのに最も簡単な方法を提供します。このフィールドは理想的にはスケジューラーによって適用され、手動のノード割り当てではなく方針に基づいて操作されます。このアプローチを通じて Pod をノードに割り当てると、その Pod は他のノードへのスケジューリングを防ぐことができます。もし指定されたノードに余裕がない、または存在しない場合、Pod は一度も実行されません。これは我々を Kubernetes 前の時代に戻し、アプリケーションを実行するためのノードを明示的に指定する必要がある状況となります。このフィールドを手動で設定することは Kubernetes のベストプラクティスではなく、例外的な状況にのみ使用すべきです。nodeSelector ノードセレクターはラベルのマップです。Pod がノードで実行されるための候補となるには、ノードのラベルとして指定されたキー-バリューのペアを持つ必要があります。すでに Pod とノードに意味のあるラベルをつけている場合（如何にせよこれを行うべきです）、ノードセレクターはスケジューラの選択を制御する最も簡単な推奨される仕組みの一つです。ノードアフィニティー このルールは手動のノード割り当ての方法を改善し、Pod が論理演算子と制約を用いてノードに対する依存性を表現することを可能にします。これにより、精緻なコントロールが可能になります。また、ノード親和性制約の厳格さをコントロールする柔軟なスケジューリング要件と厳格なスケジューリング要件を提供します。タインツと容認 タインツと容認を使用することで、ノードは既存の Pod を変更することなく、ノード上でスケジュールすべき、あるいはスケジュールすべきでない Pod を制御することができます。デフォルトでは、ノードのターレントに対して容認がない Pod は、そのノードから拒否されるか退去させられるでしょう。タインツと容認のもう一つの長所は、新しいラベル付きの新しいノードを追加して Kubernetes クラスタを拡張する場合、新しいラベルを全ての Pod に追加する必要はなく、新しいノードに配置すべき Pod のみに追加すればよいということです。一旦 Pod とノードの間の望ましい相関が Kubernetes の用語で表現されると、異なる Pod 間の依存性を特定します。密接に結合されたアプリケーションのための Pod コロケーションには Pod 親和性技術を、Pod をノードに散らばらせ、単一の故障点を避けるためには Pod 対親和性技術を使用します：Pod 親和性と対親和性 これらのルールは、Pod が他の Pod に対して出す依存性に基づくスケジューリングを可能にします。親和性ルールは少遅延とデータローカリティ要件を持つ同じトポロジ内の複数の Pod で構成される密接に結合したアプリケーションスタックを隣に配置するのに役立ちます。一方、対親和性ルールは Pod をクラスタ全体に散らばらせ、単一故障点を避けたり、リソースを集中的に利用する Pod がリソースを競合するのを防ぐために、同一のノードに配置するのを避けることができます。 トポロジースプレッド制約 これらの特徴を使用するには、プラットフォーム管理者がノードにラベルを付け、地域、ゾーン、または他のユーザー定義ドメインなどのトポロジ情報を提供する必要があります。次に、ワークロードの作成者は、ポッドの設定を作成する際に基礎となるクラスタートポロジーを認識し、トポロジースプレッド制約を指定する必要があります。ただし、これらすべての制約が満たされる必要があります。それらが相互に衝突しないことを確認してください。また、この機能を NodeAffinity および NodeSelector と組み合わせて、均等性が適用されるべきノードをフィルタリングすることもできます。その場合、違いを理解することを確認してください：複数のトポロジー・スプレッド制約は、結果セットを独立して計算し、AND 結合結果を生成するものですが、一方、NodeAffinity と NodeSelector の組み合わせでは、ノード制約の結果をフィルタリングします。 あるシナリオでは、これらのスケジューリング設定すべてが特定のスケジューリング要件を表現するのに十分な柔軟性がないかもしれません。その場合、スケジューラの設定をカスタマイズしてチューニングする、または独自のスケジューラ実装を提供する必要があるかもしれません。これにより、あなたの独自のニーズを理解することができます：スケジューラのチューニング デフォルトのスケジューラは、新しいポッドをクラスタ内のノードに配置する役割を持っており、それをうまくこなします。しかし、フィルタリングや優先順位付けのフェーズで一つ以上のステージを変更することも可能です。この機構は、エクステンションポイントとプラグインを備えており、完全な新しいスケジューラ実装を必要とせずに小さな変更を許可するために特別に設計されています。カスタムスケジューラ
複雑なスケジューリング要件がある場合、または特別なスケジューリング要件がある場合、独自のカスタムスケジューラを実装することもできます。カスタムスケジューラは、標準の Kubernetes スケジューラとは別に、またはその代わりに動作できます。ハイブリッドアプローチとしては、標準の Kubernetes スケジューラがスケジューリング決定を行う際に最終的なパスとして参照する "スケジューラエクステンダ" プロセスを持つこともできます。この方法を用いると、完全なスケジューラを提供する必要はなく、HTTP API を提供してノードをフィルタリングおよび優先順位付けするだけで済みます。自分のスケジューラを持つ利点は、ハードウェアコスト、ネットワークレイテンシ、Pod をノードに割り当てる際の利用率の向上など、Kubernetes クラスタ外の要素を考慮することができることです。また、デフォルトのスケジューラに加えて複数のカスタムスケジューラを使用し、どのスケジューラを各ポッドに使用するかを設定することも可能です。各スケジューラは、Pod の一部の集合に専用の一連のポリシーを持つことができます。
