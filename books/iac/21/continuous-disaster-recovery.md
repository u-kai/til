Continuous Disaster Recovery Iron Age infrastructure management approaches view disaster recovery as an unusual event. Recovering from the failure of static hardware often requires shifting workloads to a separate set of hardware kept
on standby. Many organizations test their recovery operation infrequently — every few months at best, in some cases once a year. I’ve seen plenty of organizations that rarely test their failover process. The assumption is that the team will work out how to get its backup system running if it ever needs to, even if it takes a few days. Continuous disaster recovery leverages the same processes and tools used to provision and change infrastructure. As described earlier, you can apply your infrastructure code to rebuild failed infrastructure, perhaps with some added automation to avoid data loss. One of the principles of Cloud Age infrastructure is to assume systems are unreliable (“Principle: Assume Systems Are Unreliable”). You can’t install software onto a virtual machine and expect it will run there as long as you like. Your cloud vendor might move, destroy, or replace the machine or its host system for maintenance, security patches, or upgrades. So you need to be ready to replace the server if needed.9 Treating disaster recovery as an extension of normal operations makes it far more reliable than treating it as an exception. Your team exercises your recovery process and tools many times a day as it works on infrastructure code changes and system updates. If someone makes a change to a script or other code that breaks provisioning or causes data loss on an update, it usually fails in a pipeline test stage, so you can quickly fix it. Chaos Engineering Netflix was a pioneer of continuous disaster recovery and Cloud Age infrastructure management.10 Its Chaos Monkey and Simian Army took the concept of continuous disaster recovery a step further, proving the effectiveness of its system’s continuity mechanisms by injection error into production systems. This evolved into the field of Chaos Engineering, “The discipline of experimenting on a system to build confidence in the system’s capability.”11 To be clear, chaos engineering is not about irresponsibly causing production service outages. Practitioners experiment with specific failure scenarios that their system is expected to handle. These are
essential production tests that prove that detection and recovery mechanisms work correctly. The intention is to gain fast feedback when some change to the system has a side effect that interferes with these mechanisms. Planning for Failure Failures are inevitable. While you can and should take steps to make them less likely, you also need to put measures in place to make them less harmful and easier to handle. A team holds a failure scenario mapping workshop to brainstorm types of failures that may occur, and then plan mitigations.12 You can create a map of likelihood and impact of each scenario, build a list of actions to address the scenarios, and then prioritize these into your team’s backlog of work appropriately. For any given failure scenario, there are several conditions to explore: Causes and prevention What situations could lead to this failure, and what can you do to make them less likely? For example, a server might run out of disk space when usage spikes. You could address this by analyzing disk usage patterns and expanding the disk size, so there is enough for the higher usage levels. You might also implement automated mechanisms to continuously analyze usage levels and make predictions, so disk space can be added preemptively if patterns change. A further step would be to automatically adjust disk capacity as usage increases. Failure mode What happens when the failure occurs? What can you do to reduce the consequence without human intervention? For example, if a given server runs out of disk space, the application running on it might accept transactions but fail to record them. This could be very harmful, so you might modify the application to stop accepting transactions if it can’t record them to disk. In many cases, teams don’t actually know what will happen when a given error occurs. Ideally, your failure mode keeps the system fully operational. For example, when an application stops responding, your load balancer may stop directing traffic to it. Detection
How will you detect when the failure occurs? What can you do to detect it faster, maybe even beforehand? You might detect that the disk has run out of space when the application crashes and a customer calls your CEO to complain. Better to receive a notification when the application crashes. Better still to be notified when disk space runs low, before it actually fills up. Correction What steps do you need to take to recover from the failure? In some scenarios, as described earlier, your systems may automatically correct the situation, perhaps by destroying and rebuilding an unresponsive application instance. Others require several steps to repair and restart a service. If your system automatically handles a failure scenario, such as restarting an unresponsive compute instance, you should consider the deeper failure scenario. Why did the instance become unresponsive in the first place? How will you detect and correct an underlying issue? It shouldn’t take you several days to realize that application instances are being recycled every few minutes. Failure planning is a continuous process. Whenever you have an incident with your system, including in a development or test environment, your team should consider whether there is a new failure scenario to define and plan for. You should implement checks to prove your failure scenarios. For example, if you believe that when your server runs out of disk space the application will stop accepting transactions, automatically add new server instances, and alert your team, you should have an automated test that exercises this scenario. You could test this in a pipeline stage (availability testing as described in “What Should We Test with Infrastructure?”) or using a chaos experiment.
Incrementally Improving Continuity It’s easy to define ambitious recovery measures, where your system gracefully handles every conceivable failure without interrupting service. I’ve never known a team that had the time and resources to build even half of what it would like. When mapping failure scenarios and mitigations, you can define an incremental set of measures you could implement. Break them into separate implementation stories and prioritize them on your backlog,
based on the likelihood of the scenario, potential damage, and cost to implement. For example, although it would be nice to automatically expand the disk space for your application when it runs low, getting an alert before it runs out is a valuable first step.

連続的な災害復旧の鉄器時代のインフラストラクチャ管理アプローチでは、災害復旧を異常なイベントと見なします。静的なハードウェアの故障からの回復には、待機している別のハードウェアセットにワークロードを移行することがしばしば必要とされます。多くの組織は、災害復旧の操作を頻繁にテストしません。最良の場合でも、数ヶ月に一度、場合によっては1年に一度です。私は、フェイルオーバープロセスをめったにテストしない組織を何度も見てきました。前提として、チームがバックアップシステムを稼働させる方法を見つけ出すだろうということです。たとえそれに数日かかったとしてもです。連続的な災害復旧は、インフラストラクチャをプロビジョニングおよび変更するために使用されるプロセスとツールと同じものを活用します。前述のように、インフラストラクチャの再構築においてインフラストラクチャコードを適用し、データの損失を避けるためにいくつかの追加の自動化を行うことができます。クラウド時代のインフラストラクチャの原則の一つは、システムは信頼性がないということです（「原則：システムは信頼性がない」という原則）。仮想マシンにソフトウェアをインストールして、思い通りに動作すると期待することはできません。クラウドベンダーは、メンテナンス、セキュリティパッチ、またはアップグレードのためにマシンまたはホストシステムを移動、破棄、または置換する可能性があります。したがって、必要に応じてサーバーを交換できる準備をしておく必要があります。災害復旧を通常の業務の一環として扱うことで、例外として扱うよりもはるかに信頼性が高くなります。チームは、インフラストラクチャコードの変更やシステムの更新に取り組む中で、毎日何度も回復プロセスとツールを練習します。スクリプトや他のコードの変更がプロビジョニングの障害やデータの損失を引き起こす場合、通常はパイプラインのテスト段階で失敗するため、迅速に修正できます。カオスエンジニアリングは、連続的な災害復旧とクラウド時代のインフラストラクチャ管理のパイオニアとしてNetflixが推進しました。そのChaos MonkeyとSimian Armyは、プロダクションシステムにエラーを注入することによって、システムの持続性メカニズムの効果を証明しました。これはカオスエンジニアリングという領域に発展し、「システムの能力に自信を持つための実験の学問」です。明確に言っておくと、カオスエンジニアリングは、過激なプロダクションサービスの停止を引き起こすことではありません。実践者は、システムが処理するはずの特定の障害シナリオで実験します。これらは正しく機能する検出および回復メカニズムを証明するための必須の本番テストです。障害への対応障害は避けられません。可能性を減らすための手段を講じることができますが、より被害を少なくし、対処が容易になるようにするための対策も講じる必要があります。チームは、障害シナリオマッピングワークショップを開催し、発生しうる障害の種類を考え、それに対する緩和策を計画します。各シナリオの発生確率と影響のマップを作成し、シナリオに対処するためのアクションのリストを作成し、それを適切にチームのバックログに優先順位付けします。特定の障害シナリオを探るために探索するべきいくつかの条件があります。原因と予防この障害につながる可能性のある状況は何か？それらをより起こりにくくするために何ができるか？たとえば、使用量が急増したときにサーバーのディスク容量が不足する可能性があります。ディスク使用量のパターンを分析し、ディスクサイズを拡張することで、これに対処することができます。さらに、使用量が変化した場合に事前にディスク容量を追加できるように、連続的に使用量を分析し予測するための自動化メカニズムを実装することもできます。障害モード障害が発生した場合に何が起こるのか？人の介入なしで結果を減らすために何ができるか？たとえば、あるサーバーがディスク容量不足になった場合、それによって実行されるアプリケーションはトランザクションを受け入れるが記録できなくなるかもしれません。これは非常に有害ですので、ディスクに記録できない場合はトランザクションの受け入れを停止するようにアプリケーションを変更することができます。多くの場合、チームは実際には指定されたエラーが発生した場合に何が起こるのかを把握していません。理想的には、障害モードはシステムを完全に運用可能な状態に保つべきです。たとえば、アプリケーションが応答しなくなると、負荷分散装置はそれにトラフィックを誘導しなくなるかもしれません。検出障害が発生した場合、どのように検出しますか？それをより早く検出するために何ができるか？顧客がアプリケーションがクラッシュしたことをCEOに訴えるまでディスクが容量不足になったことに気付くかもしれません。実際に満杯になる前にディスク容量が低下すると通知を受けることができれば、さらに良いでしょう。修正障害から回復するためにどの手順を踏む必要がありますか？前述のように、システムが自動的に状況を修正する場合、例えば、反応しないコンピュートインスタンスを再起動する場合、より深い障害シナリオを考慮する必要があります。インスタンスが最初に反応しなくなったのはなぜですか？根本的な問題を検出し、修正する方法はありますか？アプリケーションインスタンスが数分ごとに再起動されていることに数日かかる必要はありません。障害計画は連続的なプロセスです。システムに障害が発生した場合、開発環境やテスト環境などであっても、チームは新しい障害シナリオを定義し、計画する必要があります。障害シナリオが正しく機能することを証明するためのチェックを実装する必要があります。たとえば、サーバーのディスク容量が不足するとアプリケーションがトランザクションを受け入れなくなると信じている場合、新しいサーバーインスタンスを自動的に追加してチームに通知することができるようにする自動化テストを実装する必要があります。これをパイプラインのステージ（「インフラストラクチャのテストでは何をテストすべきか」で説明されている可用性テスト）またはカオス実験でテストすることができます。断片的な連続性の向上復旧対策がスムーズに機能し、サービスに中断を与えることなく、あらゆる考えられる障害に対応するシステムを定義することは容易です。私は、それを実現するための時間とリソースを持っているチームには出くわしたことがありません。障害シナリオと緩和策をマッピングする際、実装できるインクリメン